{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYWlhPoS2erA"
      },
      "source": [
        "# <b>Diabetic Retinopathy Lesion Classification using Machine Learning</b>\n",
        "\n",
        "- <b>Name: </b>Sibin Shibu\n",
        "- <b>Student ID: </b>A00014748\n",
        "- <b>Subject: </b>MSc Dissertation\n",
        "- <b>Supervisor: </b>Dr. Mohammed Farhan Khan\n",
        "- <b>Submission Date: </b> August 23, 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Pu2I9h73r9m"
      },
      "outputs": [],
      "source": [
        "!rm -r sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1bA6ZgepM1A"
      },
      "outputs": [],
      "source": [
        "# # Delete my_dir\n",
        "\n",
        "# import shutil\n",
        "# folder_path = ['myproject/processed_images/segmentation']\n",
        "\n",
        "# for folder in folder_path:\n",
        "#   shutil.rmtree(f'/content/{folder}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utv2YGIsTmeF"
      },
      "source": [
        "# <b>1. Import Required Libraries</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7PuCNJsjAOI"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Image processing library\n",
        "import cv2\n",
        "\n",
        "# data processing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EL1gF21jLap"
      },
      "source": [
        "# <b>2. Data Preparation</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERLtHEHSqYsW"
      },
      "source": [
        "## 2.1 Dataset Extraction (Unzipping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtlKDt5VEZGE"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"nguyenhung1903/diaretdb1-v21\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odNuGcSGtVsp"
      },
      "outputs": [],
      "source": [
        "# Print the top-level folder first\n",
        "print(f\"{path}\")\n",
        "\n",
        "# List its contents (1 level down)\n",
        "for item in os.listdir(path):\n",
        "    item_path = os.path.join(path, item)\n",
        "\n",
        "    # If it's a folder, go inside and list contents\n",
        "    if os.path.isdir(item_path):\n",
        "        print(f\"- {item}\")\n",
        "        for sub_item in os.listdir(item_path):\n",
        "            print(f\"  - {sub_item}\")\n",
        "    else:\n",
        "        print(f\"  - {item}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjm15Vitvpkz"
      },
      "source": [
        "## 2.2 Define Input Directories and Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_AeZAYnjx_V"
      },
      "outputs": [],
      "source": [
        "base_path = f'{path}/ddb1_v02_01'\n",
        "\n",
        "# Image Directory\n",
        "img_dir = f'{base_path}/images'\n",
        "\n",
        "# Groundtruth Directory\n",
        "gtruth_dir = f'{base_path}/groundtruth'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdUaosms_Cjz"
      },
      "source": [
        "## 2.3 Parse Ground Truth XML Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP25PHJaj_1S"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Parser function to extract annotations\n",
        "def parse_xml_annotation(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    anns = []\n",
        "    for m in root.findall('.//marking'):\n",
        "        lesion = m.findtext('markingtype')\n",
        "        conf   = m.findtext('confidencelevel')\n",
        "\n",
        "        # Representative point (if present)\n",
        "        rep_txt = m.findtext('representativepoint/coords2d')\n",
        "        rep_pt = tuple(map(int, rep_txt.split(','))) if rep_txt else None\n",
        "\n",
        "        # 1) Polygon‑based (<polygonregion>)\n",
        "        poly_region = m.find('polygonregion')\n",
        "        if poly_region is not None:\n",
        "            # centroid under polygonregion\n",
        "            c_txt = poly_region.findtext('centroid/coords2d')\n",
        "            cx, cy = map(int, c_txt.split(',')) if c_txt else (None, None)\n",
        "            # all coords2d points (including centroid)\n",
        "            polygon_pts = []\n",
        "            for coord in poly_region.findall('coords2d'):\n",
        "                xy = coord.text\n",
        "                if xy and ',' in xy:\n",
        "                    x, y = map(int, xy.split(','))\n",
        "                    polygon_pts.append((x, y))\n",
        "            anns.append({\n",
        "                'type':                lesion,\n",
        "                'centroid':            (cx, cy),\n",
        "                'representative_pt':   rep_pt,\n",
        "                'radius':              None,\n",
        "                'polygon':             polygon_pts,\n",
        "                'confidence':          conf\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # 2) Circle‑based (<circleregion>)\n",
        "        circ = m.find('circleregion')\n",
        "        if circ is not None:\n",
        "            c_txt = circ.findtext('centroid/coords2d')\n",
        "            r_txt = circ.findtext('radius')\n",
        "            if c_txt and r_txt:\n",
        "                cx, cy = map(int, c_txt.split(','))\n",
        "                r      = int(r_txt)\n",
        "                anns.append({\n",
        "                    'type':                lesion,\n",
        "                    'centroid':            (cx, cy),\n",
        "                    'representative_pt':   rep_pt,\n",
        "                    'radius':              r,\n",
        "                    'polygon':             None,\n",
        "                    'confidence':          conf\n",
        "                })\n",
        "    return anns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc1Otu2ahWVZ"
      },
      "source": [
        "## 2.4 Create Unified Annotation DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bYYYiM3hfn5"
      },
      "outputs": [],
      "source": [
        "# Build and display annotation DataFrame\n",
        "def build_annotation_dataframe():\n",
        "  records = []\n",
        "  for fname in sorted(os.listdir(gtruth_dir)):\n",
        "      if not fname.lower().endswith('.xml'):\n",
        "          continue\n",
        "\n",
        "      xml_path = os.path.join(gtruth_dir, fname)\n",
        "      # Determine image type from filename\n",
        "      img_type = 'plain' if '_plain' in fname else 'non_plain'\n",
        "      anns = parse_xml_annotation(xml_path)\n",
        "      for ann in anns:\n",
        "          records.append({\n",
        "              'gtruth_name':            fname,\n",
        "              'gtruth_type':          img_type,\n",
        "              'lesion_type':          ann['type'],\n",
        "              'centroid':             ann['centroid'],\n",
        "              'representative_point': ann['representative_pt'],\n",
        "              'radius':               ann['radius'],\n",
        "              'polygon':              ann['polygon'],\n",
        "              'confidence':           ann['confidence']\n",
        "          })\n",
        "  df = pd.DataFrame(records)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnV-Oyi41JMM"
      },
      "outputs": [],
      "source": [
        "df = build_annotation_dataframe()\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6cV9Zlmqkst"
      },
      "source": [
        "## 2.5 Link Annotations with Corresponding Raw Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouGmescswh0Y"
      },
      "outputs": [],
      "source": [
        "# Select non_plain xml points and create a copy to avoid SettingWithCopyWarning\n",
        "df = df[df['gtruth_type'] == 'non_plain'].copy()\n",
        "\n",
        "# Filter and remove irrelevant lesion classes\n",
        "df = df[~df['lesion_type'].isin(['Disc', 'Fundus_area', 'IRMA', 'Neovascularisation'])]\n",
        "\n",
        "# Derive image_name\n",
        "df['image_name'] = df['gtruth_name'].str.replace(r\"_\\d+\\.xml$\", \".png\", regex=True)\n",
        "\n",
        "# Expand coordinates safely\n",
        "df[['centroid_x', 'centroid_y']] = pd.DataFrame(df['centroid'].tolist(), index=df.index)\n",
        "df[['rep_x', 'rep_y']] = pd.DataFrame(df['representative_point'].tolist(), index=df.index)\n",
        "\n",
        "# Drop the original tuple columns\n",
        "df = df.drop(columns=['gtruth_name', 'gtruth_type', 'centroid', 'representative_point'])\n",
        "\n",
        "# Rename Lesion categories for clarity\n",
        "rename_map = {\n",
        "    \"Red_small_dots\": \"MA\",\n",
        "    \"Haemorrhages\"  : \"HE\",\n",
        "    \"Hard_exudates\" : \"EX\",\n",
        "    \"Soft_exudates\" : \"CWS\"\n",
        "}\n",
        "df['lesion_type'] = df['lesion_type'].replace(rename_map)\n",
        "\n",
        "# Organize dataframe columns\n",
        "df = df[['image_name', 'lesion_type', 'centroid_x', 'centroid_y', 'radius', 'polygon', 'rep_x', 'rep_y', 'confidence']]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TidTa9eqs7oI"
      },
      "source": [
        "## 2.6 Save Raw Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87pB6JKQIvx2"
      },
      "outputs": [],
      "source": [
        "# Save dataframe\n",
        "df.to_csv('annotations.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxOQiM2qn72I"
      },
      "source": [
        "## 2.7 Overlay Annotations on Raw Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34kA7vVwn5H8"
      },
      "outputs": [],
      "source": [
        "def overlay_annotations(df, img_dir, output_dir, default_color=(0, 255, 0)):\n",
        "  \"\"\"\n",
        "  Overlays lesion annotations (circles, polygons, points) on images and saves them.\n",
        "  \"\"\"\n",
        "\n",
        "  # Default color map\n",
        "  color_map = {\n",
        "      'MA'  : (255,   0, 255),  # Magenta\n",
        "      'HE'  : (  0,   0, 255),  # Red\n",
        "      'EX'  : (  0, 255, 255),  # Yellow\n",
        "      'CWS' : (  255, 0, 0),  # Orange\n",
        "  }\n",
        "\n",
        "  # Ensure output directory exists\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "  # Process images grouped by name\n",
        "  for img_file, group in df.groupby('image_name'):\n",
        "      img_path = os.path.join(img_dir, img_file)\n",
        "      img = cv2.imread(img_path)\n",
        "\n",
        "      if img is None:\n",
        "          print(f\"[WARNING] Missing image: {img_file}\")\n",
        "          continue\n",
        "\n",
        "      # Draw annotations\n",
        "      for _, row in group.iterrows():\n",
        "          lesion = row['lesion_type']\n",
        "          col = color_map.get(lesion, default_color)\n",
        "\n",
        "          # Circle lesions (if radius is available)\n",
        "          if pd.notnull(row.get('radius', np.nan)):\n",
        "              center = (int(row['centroid_x']), int(row['centroid_y']))\n",
        "              cv2.circle(img, center, int(row['radius']), col, 2)\n",
        "\n",
        "          # Polygon lesions\n",
        "          if isinstance(row.get('polygon', None), list):\n",
        "              pts = np.array(row['polygon'], dtype=np.int32).reshape(-1, 1, 2)\n",
        "              cv2.polylines(img, [pts], isClosed=True, color=col, thickness=2)\n",
        "\n",
        "          # Representative point (if available)\n",
        "          if pd.notnull(row.get('rep_x', np.nan)):\n",
        "              rep = (int(row['rep_x']), int(row['rep_y']))\n",
        "              cv2.circle(img, rep, 3, col, -1)\n",
        "\n",
        "      # Save annotated image\n",
        "      cv2.imwrite(os.path.join(output_dir, img_file), img)\n",
        "\n",
        "  print(\"Image annotation completed!\")\n",
        "  print(f\"Annotated images saved in: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNnsyMOXn_NB"
      },
      "outputs": [],
      "source": [
        "output_dir = 'raw_overlay_annotations'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "overlay_annotations(df, img_dir, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4223jwHKn_Jq"
      },
      "outputs": [],
      "source": [
        "def display_images(folder, cols=5, row_height=4):\n",
        "  files = [f for f in sorted(os.listdir(folder)) if f.endswith(\".png\")]\n",
        "  rows = -(-len(files) // cols)  # ceiling division\n",
        "\n",
        "  plt.figure(figsize=(20, row_height * rows))\n",
        "  for i, file in enumerate(files, 1):\n",
        "      img = cv2.cvtColor(cv2.imread(os.path.join(folder, file)), cv2.COLOR_BGR2RGB)\n",
        "      plt.subplot(rows, cols, i)\n",
        "      plt.imshow(img)\n",
        "      plt.title(file)\n",
        "      plt.axis(\"off\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHmrSb1voENz"
      },
      "outputs": [],
      "source": [
        "display_images('raw_overlay_annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DPgnqjnoJbn"
      },
      "source": [
        "# <b>3. Exploratory Data Analysis (EDA)</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl5SIZW-XBqM"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k-bCgp2XjzF"
      },
      "outputs": [],
      "source": [
        "df['lesion_type'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI2smQCLfl9T"
      },
      "source": [
        "## 3.1 Lesion Type Distribution Pie Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXUtSLd2XBoM"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Generate pie chart to show distribution of classes (labels) in the dataset.\n",
        "\"\"\"\n",
        "# Set figure size\n",
        "plt.figure(figsize=(8, 7))\n",
        "\n",
        "# Create the scatterplot\n",
        "plt.pie(df['lesion_type'].value_counts(), labels=df['lesion_type'].value_counts().index, autopct='%1.1f%%')\n",
        "\n",
        "# Add title and legend\n",
        "plt.title('Lesion Type Distribution', fontsize=14, fontweight='bold')\n",
        "plt.legend(title=\"Lesion Type\")\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs5p598NftQ-"
      },
      "source": [
        "## 3.2 Lesion Type vs Confidence Bar Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuCFvaa4bpyA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Aggregate counts of lesion_type × confidence\n",
        "counts = df.groupby([\"lesion_type\", \"confidence\"]).size().unstack(fill_value=0)\n",
        "\n",
        "# Reorder lesion types by frequency (descending)\n",
        "order = df['lesion_type'].value_counts().index\n",
        "counts = counts.reindex(order)\n",
        "\n",
        "# Choose a seaborn palette (3 colors, since you have High/Medium/Low)\n",
        "palette = sns.color_palette(\"Dark2\", n_colors=len(counts.columns))\n",
        "\n",
        "# Plot stacked bar chart with seaborn palette\n",
        "counts.plot(\n",
        "    kind=\"bar\",\n",
        "    stacked=True,\n",
        "    figsize=(10,6),\n",
        "    color=palette\n",
        ")\n",
        "\n",
        "plt.title(\"Lesion Type vs Confidence\", fontsize=14, fontweight='bold')\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Lesion Type\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title=\"Confidence\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSvVngcpfzgA"
      },
      "source": [
        "## 3.3 Annotations: Polygon vs Centroid Distribution Bar Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKP8IxgOXBl7"
      },
      "outputs": [],
      "source": [
        "# Count polygon vs centroid annotations\n",
        "polygon_counts = df['polygon'].notna().map({True: 'Polygon-based', False: 'Centroid-based'}).value_counts()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6,4))\n",
        "polygon_counts.plot(kind='bar', color=['skyblue', 'salmon'])\n",
        "\n",
        "plt.title(\"Annotations: Polygon vs Centroid\", fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Annotation Type')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKkV-bmRp5G9"
      },
      "source": [
        "## 3.4 Top 20 Images with Most Lesions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLwr-_u1p3z9"
      },
      "outputs": [],
      "source": [
        "# Group by image and lesion type\n",
        "lesion_counts = df.groupby(['image_name', 'lesion_type']).size().unstack(fill_value=0)\n",
        "\n",
        "# Select top 20 images by total lesion count\n",
        "top_20_images = lesion_counts.sum(axis=1).nlargest(20).index\n",
        "top_lesion_counts = lesion_counts.loc[top_20_images]\n",
        "\n",
        "# Plot grouped/hue bar chart\n",
        "top_lesion_counts.plot(kind='bar', stacked=True, figsize=(14, 6), colormap='tab20')\n",
        "\n",
        "plt.title('Top 20 Images with Most Lesions', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Image Name')\n",
        "plt.ylabel('Number of Lesions')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Lesion Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VM1xhw-v7R5"
      },
      "source": [
        "# <b>4. Image Pre-processing</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwqTq5FEw9gt"
      },
      "source": [
        "## 4.1 Define Processed Image Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHBoruvCo9Po"
      },
      "outputs": [],
      "source": [
        "output1 = \"preprocessed\"\n",
        "output2 = \"preprocessed_stages\"\n",
        "\n",
        "os.makedirs(output1, exist_ok=True)\n",
        "os.makedirs(output2, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGZ2LUyOm87O"
      },
      "source": [
        "## 4.2 Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLMEONLDG_2j"
      },
      "outputs": [],
      "source": [
        "# ==================== Single Image Preprocessing ====================\n",
        "def preprocess_image_with_annotations_stages(img_path, annot_df, target_size):\n",
        "    \"\"\"Preprocess one image and update its annotations while capturing stages.\"\"\"\n",
        "    image_name = os.path.basename(img_path)\n",
        "    stages = {}\n",
        "\n",
        "    # Read image\n",
        "    image = cv2.imread(img_path)\n",
        "    stages[\"Original\"] = image.copy()\n",
        "\n",
        "    # Filter annotations\n",
        "    annots = annot_df[annot_df['image_name'] == image_name].copy()\n",
        "\n",
        "    # === 1. Resize ===\n",
        "    original_w, original_h = image.shape[1], image.shape[0]\n",
        "    resized_img = cv2.resize(image, target_size)\n",
        "    scale_x = target_size[0] / original_w\n",
        "    scale_y = target_size[1] / original_h\n",
        "    stages[\"Resized\"] = resized_img.copy()\n",
        "\n",
        "    def scale_polygon(polygon):\n",
        "        if isinstance(polygon, list):\n",
        "            return [(px * scale_x, py * scale_y) for px, py in polygon]\n",
        "        return polygon\n",
        "\n",
        "    for coord in ['centroid_x', 'rep_x']:\n",
        "        annots[coord] *= scale_x\n",
        "    for coord in ['centroid_y', 'rep_y']:\n",
        "        annots[coord] *= scale_y\n",
        "\n",
        "    if 'radius' in annots.columns:\n",
        "        annots['radius'] *= (scale_x + scale_y) / 2\n",
        "\n",
        "    annots['polygon'] = annots['polygon'].apply(scale_polygon)\n",
        "\n",
        "    # === 2. Green channel ===\n",
        "    green = resized_img[:, :, 1]\n",
        "    stages[\"Green Channel\"] = green.copy()\n",
        "\n",
        "    # === 3. CLAHE ===\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    enhanced = clahe.apply(green)\n",
        "    stages[\"CLAHE\"] = enhanced.copy()\n",
        "\n",
        "    # === 4. Median Filter ===\n",
        "    denoised = cv2.medianBlur(enhanced, 3)\n",
        "    stages[\"Median Filtered\"] = denoised.copy()\n",
        "\n",
        "    # === 5. Morphological Opening ===\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    morph_cleaned = cv2.morphologyEx(denoised, cv2.MORPH_OPEN, kernel)\n",
        "    stages[\"Morphological Opening\"] = morph_cleaned.copy()\n",
        "\n",
        "    # === 6. Normalize ===\n",
        "    normalized = morph_cleaned.astype('float32') / 255.0\n",
        "    stages[\"Normalized\"] = (normalized * 255).astype(np.uint8)\n",
        "    return normalized, annots, stages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQhN8xqsorgB"
      },
      "outputs": [],
      "source": [
        "# ==================== Utility to Save Stages ====================\n",
        "def save_stages_grid(stages_dict, save_path):\n",
        "    \"\"\"Save all preprocessing stages as a single grid image.\"\"\"\n",
        "    num_images = len(stages_dict)\n",
        "    cols = 3\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    plt.figure(figsize=(12, 4 * rows))\n",
        "    for i, (stage_name, img) in enumerate(stages_dict.items()):\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        if len(img.shape) == 3:\n",
        "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "        else:\n",
        "            plt.imshow(img, cmap='gray')\n",
        "        plt.title(stage_name)\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCJmlwSon9ij"
      },
      "source": [
        "## 4.3 Save Pre-processed Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5B1FZyAIVry"
      },
      "outputs": [],
      "source": [
        "# ==================== Batch Processing ====================\n",
        "def process_all_images(img_dir, prep_path, stages_path, annot_df, target_size):\n",
        "  all_annots = []\n",
        "  for img_name in tqdm(annot_df['image_name'].unique(), desc=\"Processing images\"):\n",
        "    img_path = os.path.join(img_dir, img_name)\n",
        "\n",
        "    # Preprocess the image and annotations\n",
        "    processed_img, updated_annots, stages = preprocess_image_with_annotations_stages(img_path, annot_df, target_size)\n",
        "\n",
        "    # Save final preprocessed image\n",
        "    save_path = os.path.join(prep_path, img_name)\n",
        "    cv2.imwrite(save_path, (processed_img * 255).astype('uint8'))\n",
        "\n",
        "    # Save the stages grid\n",
        "    base_name, _ = os.path.splitext(img_name)\n",
        "    stage_save_path = os.path.join(stages_path, f\"{base_name}.png\")\n",
        "    save_stages_grid(stages, stage_save_path)\n",
        "\n",
        "    # Collect updated annotations\n",
        "    all_annots.append(updated_annots)\n",
        "\n",
        "  # Merge all updated annotations into a single DataFrame\n",
        "  final_df = pd.concat(all_annots, ignore_index=True)\n",
        "  return final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7v1XBjojct1"
      },
      "source": [
        "## 4.4 Update and Save Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-Fm45OQG_zA"
      },
      "outputs": [],
      "source": [
        "# Define target resizing size\n",
        "target_size=(512, 512)\n",
        "\n",
        "# Process all images\n",
        "df = process_all_images(img_dir, \"preprocessed\", \"preprocessed_stages\", df, target_size)\n",
        "\n",
        "# Save preprocessed annotations\n",
        "df.to_csv('preprocessed_annotations.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_images(\"preprocessed\")"
      ],
      "metadata": {
        "id": "_bqos6yVytRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 9))\n",
        "img_path = 'preprocessed_stages/diaretdb1_image005.png'\n",
        "img = cv2.imread(img_path)\n",
        "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(img_rgb)\n",
        "plt.title('Preprocessing Stages', fontsize=16, fontweight='bold')\n",
        "plt.axis('off')\n",
        "\n",
        "# Display plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GZUZto9hyvEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjHl2AwIElYW"
      },
      "source": [
        "# <b>5. Segmentation</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM2-i-nIEviK"
      },
      "source": [
        "## 5.1 Define Input Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhHXyfbUrWzC"
      },
      "outputs": [],
      "source": [
        "# Segmentation Mask Directory\n",
        "output = \"segmentation\"\n",
        "subdirs = [\"major_vessels\", \"minor_vessels\", \"optic_disc\", \"combined\", \"images\", \"overlay\"]\n",
        "\n",
        "for s in subdirs:\n",
        "  os.makedirs(os.path.join(output, s), exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZL6iymRlGjB"
      },
      "source": [
        "## 5.2 Segmentation Process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E1I0pzhEX-D"
      },
      "outputs": [],
      "source": [
        "from skimage.filters import frangi\n",
        "\n",
        "for img_name in tqdm(df['image_name'].unique()):\n",
        "  img_path = os.path.join(\"preprocessed\", img_name)\n",
        "  img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "  h, w = img.shape\n",
        "\n",
        "  # === 1. Frangi filter (vessels) ===\n",
        "  vesselness = frangi(img)\n",
        "  vesselness_norm = (vesselness - vesselness.min()) / (vesselness.max() - vesselness.min())\n",
        "\n",
        "  # === 2. Separate masks with improved thresholds ===\n",
        "  major_vessels = (vesselness_norm > 0.25).astype(np.uint8) * 255\n",
        "  minor_vessels = ((vesselness_norm > 0.1) & (vesselness_norm <= 0.25)).astype(np.uint8) * 255\n",
        "\n",
        "  # === 3. Optic disc mask ===\n",
        "  od_mask = np.zeros((h, w), dtype=np.uint8)\n",
        "  blurred = cv2.GaussianBlur(img, (15, 15), 0)\n",
        "  _, _, _, max_loc = cv2.minMaxLoc(blurred)\n",
        "  cx, cy = max_loc\n",
        "  # r = max(20, int(min(h, w) * 0.05))\n",
        "  r = int(min(h, w) * 0.08)\n",
        "  cv2.circle(od_mask, (cx, cy), r, 255, -1)\n",
        "\n",
        "  # === 4. Save individual masks ===\n",
        "  cv2.imwrite(os.path.join(output, \"major_vessels\", img_name), major_vessels)\n",
        "  cv2.imwrite(os.path.join(output, \"minor_vessels\", img_name), minor_vessels)\n",
        "  cv2.imwrite(os.path.join(output, \"optic_disc\", img_name), od_mask)\n",
        "\n",
        "  # === 5. Save combined mask ===\n",
        "  combined_mask = cv2.bitwise_or(major_vessels, minor_vessels)\n",
        "  combined_mask = cv2.bitwise_or(combined_mask, od_mask)\n",
        "\n",
        "  kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "  combined_mask = cv2.dilate(combined_mask, kernel, iterations=2)\n",
        "  cv2.imwrite(os.path.join(output, \"combined\", img_name), combined_mask)\n",
        "\n",
        "  # === 6. 0 Masking to remove background ===\n",
        "  combined_mask = (combined_mask > 0).astype(np.uint8)\n",
        "\n",
        "  cleaned = img.copy()\n",
        "  cleaned[combined_mask == 1] = 0\n",
        "  cv2.imwrite(os.path.join(output, \"images\", img_name), cleaned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z7hNvOFlX7e"
      },
      "source": [
        "## 5.3 Overlay Segmented Mask on Preprocessed Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJjHC-MIlfZ2"
      },
      "outputs": [],
      "source": [
        "for img_name in tqdm(df['image_name'].unique()):\n",
        "  img_path = os.path.join(\"preprocessed\", img_name)\n",
        "\n",
        "  img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "  img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "  major_mask = cv2.imread(os.path.join(\"segmentation\", \"major_vessels\", img_name), cv2.IMREAD_GRAYSCALE)\n",
        "  minor_mask = cv2.imread(os.path.join(\"segmentation\", \"minor_vessels\", img_name), cv2.IMREAD_GRAYSCALE)\n",
        "  od_mask = cv2.imread(os.path.join(\"segmentation\", \"optic_disc\", img_name), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "  # Dilate for better visibility\n",
        "  kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2,2))\n",
        "  major_mask = cv2.dilate(major_mask, kernel, iterations=1)\n",
        "  minor_mask = cv2.dilate(minor_mask, kernel, iterations=1)\n",
        "  od_mask = cv2.dilate(od_mask, kernel, iterations=1)\n",
        "\n",
        "  overlay = img_bgr.copy()\n",
        "  overlay[major_mask > 0] = (0, 0, 255)     # Red\n",
        "  # overlay[minor_mask > 0] = (0, 255, 255)   # Yellow\n",
        "  overlay[minor_mask > 0] = (0, 255, 0)   # Green\n",
        "  overlay[od_mask > 0] = (255, 0, 0)        # Blue\n",
        "\n",
        "  blended = cv2.addWeighted(img_bgr, 0.5, overlay, 0.8, 0)\n",
        "  cv2.imwrite(os.path.join(\"segmentation\", \"overlay\", img_name), blended)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpOBBR2ilkie"
      },
      "source": [
        "## 5.4 Display Segemntation Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQnozW4alrOm"
      },
      "source": [
        "### 5.4.1 Stages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_result(image_path, major_path, minor_path, od_path, mask_path, cleaned_path, overlay_path):\n",
        "  img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "  major = cv2.imread(major_path, cv2.IMREAD_GRAYSCALE)\n",
        "  minor = cv2.imread(minor_path, cv2.IMREAD_GRAYSCALE)\n",
        "  od = cv2.imread(od_path, cv2.IMREAD_GRAYSCALE)\n",
        "  mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "  cleaned = cv2.imread(cleaned_path, cv2.IMREAD_UNCHANGED)\n",
        "  overlay = cv2.imread(overlay_path, cv2.IMREAD_COLOR)\n",
        "  overlay = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # Bigger canvas\n",
        "  plt.figure(figsize=(14, 12))\n",
        "\n",
        "  # Preprocessed image\n",
        "  plt.subplot(3, 3, 1)\n",
        "  plt.imshow(img, cmap='gray')\n",
        "  plt.title(\"Preprocessed Image\")\n",
        "\n",
        "  # Major vessels\n",
        "  plt.subplot(3, 3, 2)\n",
        "  plt.imshow(major, cmap='gray')\n",
        "  plt.title(\"Major Vessels\")\n",
        "\n",
        "  # Minor vessels\n",
        "  plt.subplot(3, 3, 3)\n",
        "  plt.imshow(minor, cmap='gray')\n",
        "  plt.title(\"Minor Vessels\")\n",
        "\n",
        "  # Optic disc\n",
        "  plt.subplot(3, 3, 4)\n",
        "  plt.imshow(od, cmap='gray')\n",
        "  plt.title(\"Optic Disc\")\n",
        "\n",
        "  # Combined mask\n",
        "  plt.subplot(3, 3, 5)\n",
        "  plt.imshow(mask, cmap='gray')\n",
        "  plt.title(\"Combined Mask\")\n",
        "\n",
        "  # Cleaned/Segmented Image\n",
        "  plt.subplot(3, 3, 6)\n",
        "  plt.imshow(cleaned, cmap='gray')\n",
        "  plt.title(\"Cleaned Image\")\n",
        "\n",
        "  # Overlay Image\n",
        "  plt.subplot(3, 3, 7)\n",
        "  plt.imshow(overlay)\n",
        "  plt.title(\"Overlay Image\")\n",
        "\n",
        "  plt.tight_layout(pad=3.0)  # add padding between subplots\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "EXivcdXTmRwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_name = df['image_name'].unique()[1]\n",
        "visualize_result(\n",
        "  os.path.join(\"preprocessed\", img_name),\n",
        "  os.path.join(\"segmentation\", \"major_vessels\", img_name),\n",
        "  os.path.join(\"segmentation\", \"minor_vessels\", img_name),\n",
        "  os.path.join(\"segmentation\", \"optic_disc\", img_name),\n",
        "  os.path.join(\"segmentation\", \"combined\", img_name),\n",
        "  os.path.join(\"segmentation\", \"images\", img_name),\n",
        "  os.path.join(\"segmentation\", \"overlay\", img_name)\n",
        ")"
      ],
      "metadata": {
        "id": "26IPkAkMohTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O258k61wL83"
      },
      "source": [
        "### 5.4.2 Major Vessels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQWoP_0PbQJT"
      },
      "outputs": [],
      "source": [
        "display_images(\"segmentation/major_vessels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO1M29A_wUv1"
      },
      "source": [
        "### 5.4.3 Minor Vessels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jnx55LGQb0Lc"
      },
      "outputs": [],
      "source": [
        "display_images(\"segmentation/minor_vessels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvG6kAa-wYVV"
      },
      "source": [
        "### 5.4.4 Optic Disc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpCpJLipbz-0"
      },
      "outputs": [],
      "source": [
        "display_images(\"segmentation/optic_disc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xTMN_DDwcm-"
      },
      "source": [
        "### 5.4.4 Combined Mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJJgEZHUGjkM"
      },
      "outputs": [],
      "source": [
        "display_images(\"segmentation/combined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61ijvHdCwfz6"
      },
      "source": [
        "### 5.4.5 Segmented Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3E6T0qwGkAh"
      },
      "outputs": [],
      "source": [
        "display_images(\"segmentation/images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6hQMCF3wldm"
      },
      "source": [
        "### 5.4.5 Overlay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd-8z6bBHZxa"
      },
      "outputs": [],
      "source": [
        "display_images(\"segmentation/overlay\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20oa9c77IO5B"
      },
      "source": [
        "# <b>6. Lesion Labelling and Feature Extraction</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uETrseT9HBOV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from skimage.filters import threshold_otsu\n",
        "from skimage.measure import regionprops, label\n",
        "from skimage.morphology import convex_hull_image\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "from scipy.ndimage import binary_erosion as ndi_binary_erosion\n",
        "\n",
        "# ----------------- helpers -----------------\n",
        "def build_lesion_image(img_rgb, g_res):\n",
        "    \"\"\"Paper lesion image: [R_orig, G_res, B_orig].\"\"\"\n",
        "    return np.stack([img_rgb[:,:,0], g_res, img_rgb[:,:,2]], axis=-1)\n",
        "\n",
        "def rgb_to_hsv01(img_rgb):\n",
        "    \"\"\"RGB uint8 -> HSV in [0,1]. (OpenCV HSV: H in [0,179], S,V in [0,255])\"\"\"\n",
        "    hsv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
        "    hsv[...,0] /= 179.0\n",
        "    hsv[...,1] /= 255.0\n",
        "    hsv[...,2] /= 255.0\n",
        "    return hsv\n",
        "\n",
        "def masked_stats(win, mask=None):\n",
        "    \"\"\"mean, var, min, max with optional boolean mask (True=use).\"\"\"\n",
        "    if mask is not None:\n",
        "        sel = win[mask]\n",
        "    else:\n",
        "        sel = win.reshape(-1)\n",
        "    if sel.size == 0:\n",
        "        return 0.0, 0.0, 0.0, 0.0\n",
        "    return float(sel.mean()), float(sel.var()), float(sel.min()), float(sel.max())\n",
        "\n",
        "def threshold_blob(winG):\n",
        "    \"\"\"Otsu (fallback 85th pct), keep component at window center.\"\"\"\n",
        "    try:\n",
        "        t = threshold_otsu(winG)\n",
        "    except Exception:\n",
        "        t = np.percentile(winG, 85)\n",
        "    bw = (winG >= t)\n",
        "    cy, cx = bw.shape[0]//2, bw.shape[1]//2\n",
        "    lbl = label(bw, connectivity=2)\n",
        "    cc = lbl[cy, cx]\n",
        "    return (lbl == cc) if cc != 0 else np.zeros_like(bw, dtype=bool)\n",
        "\n",
        "def shape_features(bw):\n",
        "  A = float(bw.sum())\n",
        "  if A == 0:\n",
        "      return [0.0]*11\n",
        "  # perimeter via erosion border (use SciPy’s binary_erosion)\n",
        "  eroded = ndi_binary_erosion(bw.astype(bool), border_value=0)\n",
        "  edge = bw.astype(bool) ^ eroded\n",
        "  P = float(edge.sum())\n",
        "  props = regionprops(bw.astype(np.uint8))[0]\n",
        "  circ = (4.0*math.pi*A)/(P*P) if P>0 else 0.0\n",
        "  eqd  = 2.0*math.sqrt(A/math.pi)\n",
        "  hull = convex_hull_image(bw.astype(bool))\n",
        "  ch_area = float(hull.sum()) or A\n",
        "  bbox_area = max(1.0, (props.bbox[2]-props.bbox[0])*(props.bbox[3]-props.bbox[1]))\n",
        "  extent = A / bbox_area\n",
        "  return [\n",
        "      A, P, circ,\n",
        "      float(getattr(props,\"eccentricity\",0.0)),\n",
        "      float(getattr(props,\"major_axis_length\",0.0)),\n",
        "      float(getattr(props,\"minor_axis_length\",0.0)),\n",
        "      float(getattr(props,\"orientation\",0.0)),\n",
        "      (A/ch_area) if ch_area>0 else 0.0,\n",
        "      extent, eqd,\n",
        "      (P/A) if A>0 else 0.0\n",
        "  ]\n",
        "\n",
        "\n",
        "def vessel_distance_map(m_vessel):\n",
        "    \"\"\"Distance to nearest vessel (expects 1=vessel, 0=else); None -> None.\"\"\"\n",
        "    if m_vessel is None:\n",
        "        return None\n",
        "    return distance_transform_edt((m_vessel==0).astype(np.uint8)).astype(np.float32)\n",
        "\n",
        "def parse_polygon(poly):\n",
        "    \"\"\"\n",
        "    Accepts:\n",
        "      - list/array of [[x,y],...]\n",
        "      - string like '[(x1,y1),(x2,y2),...]' or 'x1 y1; x2 y2; ...'\n",
        "    Returns Nx2 float32 array or None.\n",
        "    \"\"\"\n",
        "    if poly is None or (isinstance(poly,float) and np.isnan(poly)):\n",
        "        return None\n",
        "    if isinstance(poly, (list, tuple, np.ndarray)):\n",
        "        arr = np.asarray(poly, dtype=np.float32)\n",
        "        return arr if (arr.ndim==2 and arr.shape[1]==2) else None\n",
        "    if isinstance(poly, str):\n",
        "        s = poly.strip()\n",
        "        # try literal\n",
        "        try:\n",
        "            import ast\n",
        "            v = ast.literal_eval(s)\n",
        "            arr = np.asarray(v, dtype=np.float32)\n",
        "            if arr.ndim==2 and arr.shape[1]==2: return arr\n",
        "        except Exception:\n",
        "            pass\n",
        "        # try \"x y; x y; ...\"\n",
        "        pts=[]\n",
        "        for token in s.replace(',', ' ').split(';'):\n",
        "            token=token.strip()\n",
        "            if not token: continue\n",
        "            parts = token.split()\n",
        "            if len(parts)>=2:\n",
        "                try: pts.append([float(parts[0]), float(parts[1])])\n",
        "                except: pass\n",
        "        if pts:\n",
        "            return np.asarray(pts, dtype=np.float32)\n",
        "    return None\n",
        "\n",
        "def od_info_from_mask(od_mask):\n",
        "    \"\"\"Compute OD center (x,y) and diameter from OD binary mask; return (None,None) if unavailable.\"\"\"\n",
        "    if od_mask is None:\n",
        "        return None, None\n",
        "    lbl = label((od_mask!=0).astype(np.uint8))\n",
        "    props = regionprops(lbl)\n",
        "    if not props:\n",
        "        return None, None\n",
        "    r = max(props, key=lambda p: p.area)\n",
        "    cy, cx = r.centroid   # (row, col) => (y, x)\n",
        "    diameter = max(r.major_axis_length, r.minor_axis_length)\n",
        "    return (float(cx), float(cy)), float(diameter)\n",
        "\n",
        "# ----------------- main extractor -----------------\n",
        "def extract_features(\n",
        "    img_bgr,                # OpenCV-loaded original (BGR)\n",
        "    g_res,                  # green residual (HxW, uint8) – reference size\n",
        "    image_name=\"\",\n",
        "    window=17, stride=5,\n",
        "    m_major_vessel=None,    # binary mask, same HxW as g_res (1=vessel)\n",
        "    m_minor_vessel=None,    # binary mask, same HxW as g_res\n",
        "    m_od=None,              # optic disc mask, same HxW as g_res\n",
        "    gt_df=None              # ground-truth DataFrame (optional)\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with:\n",
        "      metadata: image, x, y\n",
        "      30 features: 16 intensity (R/G/S/I mean,var,min,max) +\n",
        "                   11 shape/struct +\n",
        "                   3 context (distance_to_vessel, distance_to_OD, distance_to_OD_normalized)\n",
        "      labels: lesion, lesion_group, lesion_type (defaults set to background)\n",
        "    \"\"\"\n",
        "    # ---- sanity checks ----\n",
        "    if img_bgr is None or g_res is None:\n",
        "        raise ValueError(\"img_bgr or g_res is None. Check your paths/loads.\")\n",
        "    if g_res.ndim != 2:\n",
        "        raise ValueError(\"g_res must be a single-channel (HxW) uint8 image.\")\n",
        "\n",
        "    # --- 1) Resize raw RGB to match g_res size (no cropping) ---\n",
        "    Ht, Wt = g_res.shape[:2]\n",
        "    img_bgr = cv2.resize(img_bgr, (Wt, Ht), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Optional: verify masks sizes if provided\n",
        "    for name, m in ((\"m_major_vessel\", m_major_vessel),\n",
        "                    (\"m_minor_vessel\", m_minor_vessel),\n",
        "                    (\"m_od\", m_od)):\n",
        "        if m is not None and m.shape[:2] != (Ht, Wt):\n",
        "            raise ValueError(f\"{name} must have shape {(Ht, Wt)}, got {m.shape[:2]}.\")\n",
        "\n",
        "    # --- 2) Convert to RGB and prepare planes ---\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    H, W, _ = img_rgb.shape\n",
        "    half    = window//2\n",
        "\n",
        "    lesion = build_lesion_image(img_rgb, g_res).astype(np.uint8)\n",
        "    R, G, B = lesion[:,:,0].astype(np.float32), lesion[:,:,1].astype(np.float32), lesion[:,:,2].astype(np.float32)\n",
        "    hsv     = rgb_to_hsv01(lesion)\n",
        "    S       = hsv[:,:,1].astype(np.float32)\n",
        "    I       = ((R+G+B)/3.0).astype(np.float32)\n",
        "\n",
        "    # --- 3) Build masks (foreground) ---\n",
        "    def _bin(x): return None if x is None else (x != 0).astype(np.uint8)\n",
        "    m_major = _bin(m_major_vessel)\n",
        "    m_minor = _bin(m_minor_vessel)\n",
        "    m_od    = _bin(m_od)\n",
        "\n",
        "    if m_major is not None and m_minor is not None:\n",
        "        m_vessel = np.clip(m_major + m_minor, 0, 1)\n",
        "    elif m_major is not None:\n",
        "        m_vessel = m_major\n",
        "    else:\n",
        "        m_vessel = m_minor\n",
        "\n",
        "    m_fg = np.ones((H,W), dtype=np.uint8)\n",
        "    if m_vessel is not None:\n",
        "        m_fg[m_vessel==1] = 0\n",
        "    if m_od is not None:\n",
        "        m_fg[m_od==1] = 0\n",
        "\n",
        "    # --- 4) Context maps ---\n",
        "    DT_vessel = vessel_distance_map(m_vessel) if m_vessel is not None else None\n",
        "    od_center, od_diameter = od_info_from_mask(m_od)\n",
        "\n",
        "    # --- 5) Feature extraction ---\n",
        "    rows=[]\n",
        "    for y in range(half, H-half, stride):\n",
        "        if np.count_nonzero(m_fg[y, half:W-half]) == 0:\n",
        "            continue\n",
        "        for x in range(half, W-half, stride):\n",
        "            if m_fg[y, x] == 0:\n",
        "                continue\n",
        "\n",
        "            ys, ye = y-half, y+half+1\n",
        "            xs, xe = x-half, x+half+1\n",
        "            W_fg   = m_fg[ys:ye, xs:xe].astype(bool)\n",
        "            if W_fg.mean() < 0.30:\n",
        "                continue\n",
        "\n",
        "            WR, WG, WS, WI = R[ys:ye,xs:xe], G[ys:ye,xs:xe], S[ys:ye,xs:xe], I[ys:ye,xs:xe]\n",
        "\n",
        "            # 16 intensity\n",
        "            feats=[]\n",
        "            for Wp in (WR, WG, WS, WI):\n",
        "                feats.extend(masked_stats(Wp, W_fg))\n",
        "\n",
        "            # 11 shape\n",
        "            try:\n",
        "                vals = WG[W_fg] if W_fg.any() else WG\n",
        "                t = threshold_otsu(vals)\n",
        "            except Exception:\n",
        "                vals = WG[W_fg] if W_fg.any() else WG\n",
        "                t = np.percentile(vals, 85)\n",
        "            BW = (WG >= t)\n",
        "            cy, cx = BW.shape[0]//2, BW.shape[1]//2\n",
        "            lbl = label(BW, connectivity=2)\n",
        "            cc  = lbl[cy, cx]\n",
        "            BW  = (lbl == cc) if cc != 0 else np.zeros_like(BW, dtype=bool)\n",
        "            struct = shape_features(BW)\n",
        "\n",
        "            # 3 context\n",
        "            d_vessel = float(DT_vessel[y,x]) if DT_vessel is not None else 0.0\n",
        "            if od_center is not None and od_diameter and od_diameter>0:\n",
        "                dx, dy = float(x-od_center[0]), float(y-od_center[1])\n",
        "                d_od = float(math.hypot(dx,dy))\n",
        "                d_od_norm = d_od/float(od_diameter)\n",
        "            else:\n",
        "                d_od, d_od_norm = 0.0, 0.0\n",
        "\n",
        "            # assemble row\n",
        "            row = {\"image\": image_name, \"x\": int(x), \"y\": int(y)}\n",
        "            planes=[\"R\",\"G\",\"S\",\"I\"]; stats=[\"mean\",\"var\",\"min\",\"max\"]\n",
        "            for pi,p in enumerate(planes):\n",
        "                for si,s in enumerate(stats):\n",
        "                    row[f\"{p}_{s}\"] = feats[pi*4+si]\n",
        "            names=[\"area\",\"perimeter\",\"circularity\",\"eccentricity\",\n",
        "                   \"major_axis\",\"minor_axis\",\"orientation\",\n",
        "                   \"solidity\",\"extent\",\"equiv_diam\",\"perim_area_ratio\"]\n",
        "            for n,v in zip(names, struct):\n",
        "                row[n]=v\n",
        "            row[\"distance_to_vessel\"] = d_vessel\n",
        "            row[\"distance_to_OD\"] = d_od\n",
        "            row[\"distance_to_OD_normalized\"] = d_od_norm\n",
        "\n",
        "            # default labels\n",
        "            row[\"lesion\"] = \"not_lesion\"\n",
        "            row[\"lesion_group\"] = \"background\"\n",
        "            row[\"lesion_type\"] = \"background\"\n",
        "\n",
        "            rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # --- 6) Label with GT (polygon → circle), filtered by image_name ---\n",
        "    if gt_df is not None and len(df):\n",
        "        gt_rows = gt_df[gt_df[\"image_name\"] == image_name]\n",
        "        if len(gt_rows):\n",
        "            xs = df[\"x\"].to_numpy(); ys = df[\"y\"].to_numpy()\n",
        "            for _, gt in gt_rows.iterrows():\n",
        "                lesion_type = gt.get(\"lesion_type\", \"background\")\n",
        "                group = \"red\" if lesion_type in (\"MA\",\"HE\") else (\"bright\" if lesion_type in (\"EX\",\"CWS\") else \"background\")\n",
        "                inside = None\n",
        "                poly = parse_polygon(gt.get(\"polygon\", None))\n",
        "                if poly is not None and len(poly) >= 3:\n",
        "                    poly_i = poly.astype(np.int32).reshape(-1,1,2)\n",
        "                    poly_mask = np.zeros((H,W), dtype=np.uint8)\n",
        "                    cv2.fillPoly(poly_mask, [poly_i], 1)\n",
        "                    inside = poly_mask[ys, xs].astype(bool)\n",
        "                if inside is None or not inside.any():\n",
        "                    cx = float(gt.get(\"centroid_x\", np.nan))\n",
        "                    cy = float(gt.get(\"centroid_y\", np.nan))\n",
        "                    r  = float(gt.get(\"radius\", 0.0))\n",
        "                    if not (np.isnan(cx) or np.isnan(cy) or r<=0):\n",
        "                        inside = ((xs - cx)**2 + (ys - cy)**2) <= (r*r)\n",
        "                    else:\n",
        "                        inside = np.zeros_like(xs, dtype=bool)\n",
        "                if inside.any():\n",
        "                    df.loc[inside, \"lesion\"]       = \"lesion\"\n",
        "                    df.loc[inside, \"lesion_group\"] = group\n",
        "                    df.loc[inside, \"lesion_type\"]  = lesion_type\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S89iKW1AR4Gj"
      },
      "outputs": [],
      "source": [
        "df_list = []\n",
        "for img_name in tqdm(df['image_name'].unique()):\n",
        "  # raw RGB fundus\n",
        "  img_path   = f\"{img_dir}/{img_name}\"\n",
        "\n",
        "  # green residual\n",
        "  g_res_path = f\"segmentation/images/{img_name}\"\n",
        "\n",
        "  # Load\n",
        "  img_bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "  g_res   = cv2.imread(g_res_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "  # masks\n",
        "  m_major_vessel = cv2.imread(f\"segmentation/major_vessels/{img_name}\", cv2.IMREAD_GRAYSCALE)\n",
        "  m_minor_vessel = cv2.imread(f\"segmentation/minor_vessels/{img_name}\", cv2.IMREAD_GRAYSCALE)\n",
        "  m_od           = cv2.imread(f\"segmentation/optic_disc/{img_name}\", cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "  # Ground truth DataFrame (full table)\n",
        "  gt_df = df\n",
        "\n",
        "  df_feat = extract_features(\n",
        "      img_bgr=img_bgr,\n",
        "      g_res=g_res,\n",
        "      image_name=img_name,\n",
        "      window=17,\n",
        "      stride=5,\n",
        "      m_major_vessel=m_major_vessel,\n",
        "      m_minor_vessel=m_minor_vessel,\n",
        "      m_od=m_od,\n",
        "      gt_df=gt_df\n",
        "  )\n",
        "\n",
        "  df_list.append(df_feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWvydzNASMKG"
      },
      "outputs": [],
      "source": [
        "df_main = pd.concat(df_list)\n",
        "df_main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeTgNRT-M3X6"
      },
      "outputs": [],
      "source": [
        "df_main.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up8utpdsJt5A"
      },
      "outputs": [],
      "source": [
        "df_main.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv_xgwHLJt0j"
      },
      "outputs": [],
      "source": [
        "df_main['lesion'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIOOASaJMl8P"
      },
      "outputs": [],
      "source": [
        "df_main['lesion_group'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq1GA6SwMqVI"
      },
      "outputs": [],
      "source": [
        "df_main['lesion_type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apzeOtFfYFDp"
      },
      "outputs": [],
      "source": [
        "df_main.to_csv(\"df_main_stride_5.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>7. Model Building</b>"
      ],
      "metadata": {
        "id": "37HFx52y8Nhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 Bright"
      ],
      "metadata": {
        "id": "C-xVE1ff8P90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1.1 Bright vs Background"
      ],
      "metadata": {
        "id": "EaEZ74uh8StT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression"
      ],
      "metadata": {
        "id": "qgUvEr4_Qjyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 1 — Bright vs Background (Logistic Regression) =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                             roc_auc_score, average_precision_score,\n",
        "                             ConfusionMatrixDisplay, roc_curve, auc)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "N_BG_BRIGHT = 50_000  # undersample size for background (tweak if needed)\n",
        "\n",
        "# 1) Build dataset (undersample background + all bright), make target, shuffle\n",
        "df_bg = df_main[df_main['lesion_group'] == 'background']\n",
        "df_bright = df_main[df_main['lesion_group'] == 'bright']\n",
        "df_bg_s = df_bg.sample(n=min(N_BG_BRIGHT, len(df_bg)), random_state=SEED)\n",
        "\n",
        "df_s1_bright = pd.concat([df_bg_s, df_bright], ignore_index=True)\n",
        "df_s1_bright['target'] = np.where(df_s1_bright['lesion_group'] == 'bright', 'bright', 'background')\n",
        "df_s1_bright = df_s1_bright.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# 2) Leakage-safe split by image\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
        "train_idx, test_idx = next(gss.split(df_s1_bright, groups=df_s1_bright['image']))\n",
        "train_df = df_s1_bright.iloc[train_idx].copy()\n",
        "test_df  = df_s1_bright.iloc[test_idx].copy()\n",
        "\n",
        "# 3) Features / labels\n",
        "drop_cols = ['image','x','y','lesion','lesion_group','lesion_type','target']\n",
        "feat_cols = [c for c in df_s1_bright.columns if c not in drop_cols]\n",
        "X_train, y_train = train_df[feat_cols], train_df['target']\n",
        "X_test,  y_test  = test_df[feat_cols],  test_df['target']\n",
        "\n",
        "# 4) Model: Impute + Scale + Logistic Regression (balanced)\n",
        "lr_stage1_bright = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='median')),\n",
        "    ('scale', StandardScaler()),\n",
        "    ('clf', LogisticRegression(\n",
        "        solver='saga', max_iter=200, n_jobs=-1, class_weight='balanced',\n",
        "        C=1.0, random_state=SEED\n",
        "    ))\n",
        "])\n",
        "\n",
        "lr_stage1_bright.fit(X_train, y_train)\n",
        "\n",
        "# 5) Scores (bright = positive), predictions at 0.5\n",
        "bright_idx = list(lr_stage1_bright.named_steps['clf'].classes_).index('bright')\n",
        "y_scores = lr_stage1_bright.predict_proba(X_test)[:, bright_idx]\n",
        "y_pred = np.where(y_scores >= 0.5, 'bright', 'background')\n",
        "\n",
        "# 6) Metrics (bright positive)\n",
        "y_true_bin = (y_test == 'bright').astype(int)\n",
        "y_pred_bin = (y_pred == 'bright').astype(int)\n",
        "tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin).ravel()\n",
        "\n",
        "sensitivity = tp / (tp + fn) if (tp + fn) else 0.0   # recall for bright\n",
        "specificity = tn / (tn + fp) if (tn + fp) else 0.0   # TNR for background\n",
        "accuracy   = (tp + tn) / (tp + tn + fp + fn)\n",
        "roc_auc    = roc_auc_score(y_true_bin, y_scores)\n",
        "pr_auc     = average_precision_score(y_true_bin, y_scores)\n",
        "\n",
        "print(\"\\n=== Stage 1 — Bright vs Background (LogReg) ===\")\n",
        "print(f\"Sensitivity (bright+): {sensitivity:.4f}\")\n",
        "print(f\"Specificity (background): {specificity:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC (bright+): {pr_auc:.4f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# 7) Confusion matrix plots (counts + normalized)\n",
        "labels = ['background','bright']\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(values_format='d')\n",
        "plt.title('Stage 1 Bright — Confusion Matrix (counts)')\n",
        "plt.show()\n",
        "\n",
        "cm_norm = confusion_matrix(y_test, y_pred, labels=labels, normalize='true')\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=labels).plot(values_format='.2f')\n",
        "plt.title('Stage 1 Bright — Confusion Matrix (row-normalized)')\n",
        "plt.show()\n",
        "\n",
        "# 8) ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_true_bin, y_scores)\n",
        "roc_auc_val = auc(fpr, tpr)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {roc_auc_val:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--', linewidth=1)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Stage 1 Bright — ROC Curve (Logistic Regression)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 9) Save for model comparison later\n",
        "results_lr_b1 = {\n",
        "    'stage': 'Stage 1 — Bright vs Background',\n",
        "    'model': 'LogReg',\n",
        "    'sensitivity': sensitivity,\n",
        "    'specificity': specificity,\n",
        "    'accuracy': accuracy,\n",
        "    'roc_auc': roc_auc,\n",
        "    'pr_auc': pr_auc,\n",
        "    'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
        "}"
      ],
      "metadata": {
        "id": "Ty2g_-l3PZzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest"
      ],
      "metadata": {
        "id": "S2-yfMDoQeKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 1 — Bright vs Background (Random Forest) =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                             roc_auc_score, average_precision_score,\n",
        "                             ConfusionMatrixDisplay, roc_curve, auc)\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "N_BG_BRIGHT = 50_000   # undersample background size (tweak for speed/balance)\n",
        "\n",
        "# 1) Build dataset (undersample background + all bright), make target, shuffle\n",
        "df_bg = df_main[df_main['lesion_group'] == 'background']\n",
        "df_bright = df_main[df_main['lesion_group'] == 'bright']\n",
        "df_bg_s = df_bg.sample(n=min(N_BG_BRIGHT, len(df_bg)), random_state=SEED)\n",
        "\n",
        "df_s1_bright = pd.concat([df_bg_s, df_bright], ignore_index=True)\n",
        "df_s1_bright['target'] = np.where(df_s1_bright['lesion_group'] == 'bright', 'bright', 'background')\n",
        "df_s1_bright = df_s1_bright.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# 2) Leakage-safe split by image\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
        "train_idx, test_idx = next(gss.split(df_s1_bright, groups=df_s1_bright['image']))\n",
        "train_df = df_s1_bright.iloc[train_idx].copy()\n",
        "test_df  = df_s1_bright.iloc[test_idx].copy()\n",
        "\n",
        "# 3) Features / labels\n",
        "drop_cols = ['image','x','y','lesion','lesion_group','lesion_type','target']\n",
        "feat_cols = [c for c in df_s1_bright.columns if c not in drop_cols]\n",
        "X_train, y_train = train_df[feat_cols], train_df['target']\n",
        "X_test,  y_test  = test_df[feat_cols],  test_df['target']\n",
        "\n",
        "# 4) Model: Impute -> RandomForest (trees don’t need scaling)\n",
        "rf_stage1_bright = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='median')),\n",
        "    ('clf', RandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=None,                 # set to e.g. 16 if you want extra regularization\n",
        "        min_samples_leaf=1,\n",
        "        n_jobs=-1,\n",
        "        class_weight='balanced_subsample',\n",
        "        random_state=SEED\n",
        "    ))\n",
        "])\n",
        "\n",
        "rf_stage1_bright.fit(X_train, y_train)\n",
        "\n",
        "# 5) Scores & predictions (bright = positive, threshold 0.5)\n",
        "bright_idx = list(rf_stage1_bright.named_steps['clf'].classes_).index('bright')\n",
        "y_scores = rf_stage1_bright.predict_proba(X_test)[:, bright_idx]\n",
        "y_pred = np.where(y_scores >= 0.5, 'bright', 'background')\n",
        "\n",
        "# 6) Metrics (bright positive)\n",
        "y_true_bin = (y_test == 'bright').astype(int)\n",
        "y_pred_bin = (y_pred == 'bright').astype(int)\n",
        "tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin).ravel()\n",
        "\n",
        "sensitivity = tp / (tp + fn) if (tp + fn) else 0.0   # recall for bright\n",
        "specificity = tn / (tn + fp) if (tn + fp) else 0.0   # TNR for background\n",
        "accuracy   = (tp + tn) / (tp + tn + fp + fn)\n",
        "roc_auc    = roc_auc_score(y_true_bin, y_scores)\n",
        "pr_auc     = average_precision_score(y_true_bin, y_scores)\n",
        "\n",
        "print(\"\\n=== Stage 1 — Bright vs Background (Random Forest) ===\")\n",
        "print(f\"Sensitivity (bright+): {sensitivity:.4f}\")\n",
        "print(f\"Specificity (background): {specificity:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC (bright+): {pr_auc:.4f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# 7) Confusion matrix plots (counts + normalized)\n",
        "labels = ['background','bright']\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(values_format='d')\n",
        "plt.title('Stage 1 Bright — Confusion Matrix (counts)')\n",
        "plt.show()\n",
        "\n",
        "cm_norm = confusion_matrix(y_test, y_pred, labels=labels, normalize='true')\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=labels).plot(values_format='.2f')\n",
        "plt.title('Stage 1 Bright — Confusion Matrix (row-normalized)')\n",
        "plt.show()\n",
        "\n",
        "# 8) ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_true_bin, y_scores)\n",
        "roc_auc_val = auc(fpr, tpr)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {roc_auc_val:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--', linewidth=1)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Stage 1 Bright — ROC Curve (Random Forest)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 9) Save for model comparison later\n",
        "results_rf_b1 = {\n",
        "    'stage': 'Stage 1 — Bright vs Background',\n",
        "    'model': 'RandomForest',\n",
        "    'sensitivity': sensitivity,\n",
        "    'specificity': specificity,\n",
        "    'accuracy': accuracy,\n",
        "    'roc_auc': roc_auc,\n",
        "    'pr_auc': pr_auc,\n",
        "    'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
        "}\n"
      ],
      "metadata": {
        "id": "xXPytXdYQd3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGBoost"
      ],
      "metadata": {
        "id": "bNXeVrcJQrvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 1 — Bright vs Background (XGBoost — simple) =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, roc_auc_score,\n",
        "    average_precision_score, ConfusionMatrixDisplay, roc_curve, auc\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "N_BG_BRIGHT = 50_000  # undersample background size (tweak for speed/balance)\n",
        "\n",
        "# 1) Build dataset (undersample background + all bright), make target, shuffle\n",
        "df_bg = df_main[df_main['lesion_group'] == 'background']\n",
        "df_bright = df_main[df_main['lesion_group'] == 'bright']\n",
        "df_bg_s = df_bg.sample(n=min(N_BG_BRIGHT, len(df_bg)), random_state=SEED)\n",
        "\n",
        "df_s1_bright = pd.concat([df_bg_s, df_bright], ignore_index=True)\n",
        "df_s1_bright['target'] = np.where(df_s1_bright['lesion_group']=='bright','bright','background')\n",
        "df_s1_bright = df_s1_bright.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# 2) Leakage-safe split by image\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
        "train_idx, test_idx = next(gss.split(df_s1_bright, groups=df_s1_bright['image']))\n",
        "train_df = df_s1_bright.iloc[train_idx].copy()\n",
        "test_df  = df_s1_bright.iloc[test_idx].copy()\n",
        "\n",
        "# 3) Features / labels\n",
        "drop_cols = ['image','x','y','lesion','lesion_group','lesion_type','target']\n",
        "feat_cols = [c for c in df_s1_bright.columns if c not in drop_cols]\n",
        "\n",
        "X_train_full, y_train_full_lbl = train_df[feat_cols], train_df['target']\n",
        "X_test,         y_test_lbl     = test_df[feat_cols],  test_df['target']\n",
        "\n",
        "# Binarize labels for XGBoost (bright=1, background=0)\n",
        "y_train = (y_train_full_lbl == 'bright').astype(int).values\n",
        "y_test  = (y_test_lbl == 'bright').astype(int).values\n",
        "\n",
        "# 4) Impute NaNs (trees don't need scaling)\n",
        "imp = SimpleImputer(strategy='median')\n",
        "X_train_np = imp.fit_transform(X_train_full)\n",
        "X_test_np  = imp.transform(X_test)\n",
        "\n",
        "# 5) Class imbalance handling: scale_pos_weight = (neg/pos) on TRAIN\n",
        "pos = int(y_train.sum())\n",
        "neg = int(len(y_train) - pos)\n",
        "scale_pos_weight = (neg / pos) if pos > 0 else 1.0\n",
        "print(f\"scale_pos_weight (train): {scale_pos_weight:.3f}  (neg={neg}, pos={pos})\")\n",
        "\n",
        "# 6) Model (simple, version-friendly)\n",
        "xgb_stage1_bright = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    max_depth=6,\n",
        "    n_estimators=400,          # keep modest; raise if you want\n",
        "    learning_rate=0.10,        # a bit higher since no early stopping\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    n_jobs=-1,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# 7) Fit\n",
        "xgb_stage1_bright.fit(X_train_np, y_train)\n",
        "\n",
        "# 8) Scores & predictions (bright = positive)\n",
        "y_scores = xgb_stage1_bright.predict_proba(X_test_np)[:, 1]\n",
        "y_pred   = (y_scores >= 0.5).astype(int)  # 1=bright, 0=background\n",
        "\n",
        "# 9) Metrics\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "sensitivity = tp / (tp + fn) if (tp + fn) else 0.0   # recall for bright\n",
        "specificity = tn / (tn + fp) if (tn + fp) else 0.0   # TNR for background\n",
        "accuracy   = (tp + tn) / (tp + tn + fp + fn)\n",
        "roc_auc    = roc_auc_score(y_test, y_scores)\n",
        "pr_auc     = average_precision_score(y_test, y_scores)\n",
        "\n",
        "print(\"\\n=== Stage 1 — Bright vs Background (XGBoost — simple) ===\")\n",
        "print(f\"Sensitivity (bright+): {sensitivity:.4f}\")\n",
        "print(f\"Specificity (background): {specificity:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC (bright+): {pr_auc:.4f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(\n",
        "    pd.Series(np.where(y_test==1,'bright','background')),\n",
        "    pd.Series(np.where(y_pred==1,'bright','background')),\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# 10) Confusion matrix (counts + normalized)\n",
        "labels = ['background','bright']\n",
        "cm = confusion_matrix(np.where(y_test==1,'bright','background'),\n",
        "                      np.where(y_pred==1,'bright','background'),\n",
        "                      labels=labels)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(values_format='d')\n",
        "plt.title('Stage 1 Bright — Confusion Matrix (counts, XGBoost)')\n",
        "plt.show()\n",
        "\n",
        "cm_norm = confusion_matrix(np.where(y_test==1,'bright','background'),\n",
        "                           np.where(y_pred==1,'bright','background'),\n",
        "                           labels=labels, normalize='true')\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=labels).plot(values_format='.2f')\n",
        "plt.title('Stage 1 Bright — Confusion Matrix (row-normalized, XGBoost)')\n",
        "plt.show()\n",
        "\n",
        "# 11) ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
        "roc_auc_val = auc(fpr, tpr)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {roc_auc_val:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--', linewidth=1)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Stage 1 Bright — ROC Curve (XGBoost)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 12) Save for model comparison later\n",
        "results_xgb_b1 = {\n",
        "    'stage': 'Stage 1 — Bright vs Background',\n",
        "    'model': 'XGBoost',\n",
        "    'sensitivity': sensitivity,\n",
        "    'specificity': specificity,\n",
        "    'accuracy': accuracy,\n",
        "    'roc_auc': roc_auc,\n",
        "    'pr_auc': pr_auc,\n",
        "    'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
        "}\n"
      ],
      "metadata": {
        "id": "9awRJJaZQttm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Comparison"
      ],
      "metadata": {
        "id": "FqtMbVvajkjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Collect your saved result dicts here\n",
        "rows = [results_lr_b1, results_rf_b1, results_xgb_b1]  # LR / RF / XGB (Stage 1 Bright)\n",
        "\n",
        "# Build comparison table\n",
        "tbl = pd.DataFrame([\n",
        "    {\n",
        "        \"Model\": r[\"model\"],\n",
        "        \"Sensitivity (bright)\": r[\"sensitivity\"],\n",
        "        \"Specificity (background)\": r[\"specificity\"],\n",
        "        \"Balanced Accuracy\": 0.5 * (r[\"sensitivity\"] + r[\"specificity\"]),\n",
        "        \"Accuracy\": r[\"accuracy\"],\n",
        "        \"ROC AUC\": r[\"roc_auc\"],\n",
        "        \"PR AUC (bright)\": r[\"pr_auc\"],\n",
        "        \"TP\": r[\"tp\"], \"FP\": r[\"fp\"], \"TN\": r[\"tn\"], \"FN\": r[\"fn\"]\n",
        "    }\n",
        "    for r in rows\n",
        "]).round(4).sort_values(\"ROC AUC\", ascending=False)\n",
        "\n",
        "print(tbl)\n",
        "\n",
        "# Optional: pretty display (Jupyter)\n",
        "try:\n",
        "    display(tbl.style.highlight_max(subset=[\"Sensitivity (bright)\", \"Specificity (background)\",\n",
        "                                            \"Balanced Accuracy\", \"Accuracy\", \"ROC AUC\", \"PR AUC (bright)\"],\n",
        "                                    color=\"#d5f5e3\"))\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Export for your report\n",
        "tbl.to_csv(\"stage1_bright_model_comparison.csv\", index=False)\n",
        "print(\"\\nLaTeX (paste into your paper):\\n\")\n",
        "print(tbl.to_latex(index=False, float_format=\"%.4f\", caption=\"Stage 1 (Bright vs Background) — Model comparison\",\n",
        "                   label=\"tab:s1_bright_models\"))\n"
      ],
      "metadata": {
        "id": "6OA3lDfKTYNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1.2 Bright: EX vs CWS"
      ],
      "metadata": {
        "id": "NsOhfA-sCzih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression"
      ],
      "metadata": {
        "id": "dJZ3ww51UR-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 2 — Bright (EX vs CWS) — Logistic Regression =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                             roc_auc_score, average_precision_score,\n",
        "                             ConfusionMatrixDisplay, roc_curve, auc)\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "EX_TO_CWS_RATIO_IN_TRAIN = 2.0   # set None to skip train-only rebalance\n",
        "\n",
        "# 1) Filter: bright-only, EX vs CWS\n",
        "b2 = df_main[df_main['lesion_group']=='bright'].copy()\n",
        "b2 = b2[b2['lesion_type'].isin(['EX','CWS'])].copy()\n",
        "b2['target'] = b2['lesion_type']\n",
        "\n",
        "# 2) Group-safe split by image\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
        "train_idx, test_idx = next(gss.split(b2, groups=b2['image']))\n",
        "train_df, test_df = b2.iloc[train_idx].copy(), b2.iloc[test_idx].copy()\n",
        "\n",
        "# optional: ensure both classes appear in test; reshuffle once if not\n",
        "if test_df['target'].nunique() < 2:\n",
        "    train_idx, test_idx = next(GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED+1)\n",
        "                               .split(b2, groups=b2['image']))\n",
        "    train_df, test_df = b2.iloc[train_idx].copy(), b2.iloc[test_idx].copy()\n",
        "\n",
        "# 3) Train-only rebalance (undersample EX)\n",
        "if EX_TO_CWS_RATIO_IN_TRAIN is not None:\n",
        "    ex_tr  = train_df[train_df['target']=='EX']\n",
        "    cws_tr = train_df[train_df['target']=='CWS']\n",
        "    target_ex = int(min(len(ex_tr), EX_TO_CWS_RATIO_IN_TRAIN * len(cws_tr))) if len(cws_tr)>0 else len(ex_tr)\n",
        "    ex_down = ex_tr.sample(n=target_ex, random_state=SEED) if len(ex_tr) > target_ex else ex_tr\n",
        "    train_df = pd.concat([ex_down, cws_tr], ignore_index=True).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# 4) Features / labels\n",
        "drop_cols = ['image','x','y','lesion','lesion_group','lesion_type','target']\n",
        "feat_cols = [c for c in b2.columns if c not in drop_cols]\n",
        "X_train, y_train = train_df[feat_cols], train_df['target']\n",
        "X_test,  y_test  = test_df[feat_cols],  test_df['target']\n",
        "\n",
        "# 5) Model: Impute + Scale + Logistic Regression\n",
        "lr_stage2_bright = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='median')),\n",
        "    ('scale', StandardScaler()),\n",
        "    ('clf', LogisticRegression(\n",
        "        solver='lbfgs', max_iter=1000, class_weight='balanced',\n",
        "        C=1.0, random_state=SEED\n",
        "    ))\n",
        "])\n",
        "\n",
        "lr_stage2_bright.fit(X_train, y_train)\n",
        "\n",
        "# 6) Scores & predictions (EX = positive)\n",
        "ex_idx = list(lr_stage2_bright.named_steps['clf'].classes_).index('EX')\n",
        "y_scores = lr_stage2_bright.predict_proba(X_test)[:, ex_idx]\n",
        "y_pred   = np.where(y_scores >= 0.5, 'EX', 'CWS')\n",
        "\n",
        "# 7) Metrics\n",
        "y_true_bin = (y_test == 'EX').astype(int)\n",
        "y_pred_bin = (y_pred == 'EX').astype(int)\n",
        "tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin).ravel()\n",
        "sensitivity = tp/(tp+fn) if (tp+fn) else 0.0    # EX recall\n",
        "specificity = tn/(tn+fp) if (tn+fp) else 0.0    # CWS TNR\n",
        "accuracy   = (tp+tn)/(tp+tn+fp+fn)\n",
        "roc_auc    = roc_auc_score(y_true_bin, y_scores)\n",
        "pr_auc     = average_precision_score(y_true_bin, y_scores)\n",
        "\n",
        "print(\"\\n=== Stage 2 — Bright (EX vs CWS) — Logistic Regression ===\")\n",
        "print(f\"Sensitivity (EX+): {sensitivity:.4f}\")\n",
        "print(f\"Specificity (CWS): {specificity:.4f}\")\n",
        "print(f\"Accuracy:          {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC:           {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC (EX+):      {pr_auc:.4f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# 8) Confusion matrices\n",
        "labels = ['CWS','EX']  # order shown\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(values_format='d')\n",
        "plt.title('Stage 2 Bright — Confusion Matrix (counts, LR)')\n",
        "plt.show()\n",
        "\n",
        "cm_norm = confusion_matrix(y_test, y_pred, labels=labels, normalize='true')\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=labels).plot(values_format='.2f')\n",
        "plt.title('Stage 2 Bright — Confusion Matrix (row-normalized, LR)')\n",
        "plt.show()\n",
        "\n",
        "# 9) ROC\n",
        "fpr, tpr, _ = roc_curve(y_true_bin, y_scores)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {roc_auc:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--', linewidth=1)\n",
        "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
        "plt.title('Stage 2 Bright — ROC (LR)')\n",
        "plt.legend(loc='lower right'); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# 10) Save results dict\n",
        "results_lr_b2 = {\n",
        "    'stage':'Stage 2 — Bright (EX vs CWS)', 'model':'LogReg',\n",
        "    'sensitivity':sensitivity,'specificity':specificity,'accuracy':accuracy,\n",
        "    'roc_auc':roc_auc,'pr_auc':pr_auc,'tp':tp,'tn':tn,'fp':fp,'fn':fn\n",
        "}\n"
      ],
      "metadata": {
        "id": "TxRTlMLMUOLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest"
      ],
      "metadata": {
        "id": "-FGHip7lUR69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 2 — Bright (EX vs CWS) — Random Forest =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                             roc_auc_score, average_precision_score,\n",
        "                             ConfusionMatrixDisplay, roc_curve, auc)\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "EX_TO_CWS_RATIO_IN_TRAIN = 2.0   # set None to skip\n",
        "\n",
        "# 1) Data (bright-only, EX vs CWS)\n",
        "b2 = df_main[df_main['lesion_group']=='bright'].copy()\n",
        "b2 = b2[b2['lesion_type'].isin(['EX','CWS'])].copy()\n",
        "b2['target'] = b2['lesion_type']\n",
        "\n",
        "# 2) Group-safe split\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
        "train_idx, test_idx = next(gss.split(b2, groups=b2['image']))\n",
        "train_df, test_df = b2.iloc[train_idx].copy(), b2.iloc[test_idx].copy()\n",
        "if test_df['target'].nunique() < 2:\n",
        "    train_idx, test_idx = next(GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED+1)\n",
        "                               .split(b2, groups=b2['image']))\n",
        "    train_df, test_df = b2.iloc[train_idx].copy(), b2.iloc[test_idx].copy()\n",
        "\n",
        "# 3) Train-only rebalance (EX downsample)\n",
        "if EX_TO_CWS_RATIO_IN_TRAIN is not None:\n",
        "    ex_tr  = train_df[train_df['target']=='EX']\n",
        "    cws_tr = train_df[train_df['target']=='CWS']\n",
        "    target_ex = int(min(len(ex_tr), EX_TO_CWS_RATIO_IN_TRAIN * len(cws_tr))) if len(cws_tr)>0 else len(ex_tr)\n",
        "    ex_down = ex_tr.sample(n=target_ex, random_state=SEED) if len(ex_tr) > target_ex else ex_tr\n",
        "    train_df = pd.concat([ex_down, cws_tr], ignore_index=True).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# 4) Features / labels\n",
        "drop_cols = ['image','x','y','lesion','lesion_group','lesion_type','target']\n",
        "feat_cols = [c for c in b2.columns if c not in drop_cols]\n",
        "X_train, y_train = train_df[feat_cols], train_df['target']\n",
        "X_test,  y_test  = test_df[feat_cols],  test_df['target']\n",
        "\n",
        "# 5) Model: Impute -> RandomForest\n",
        "rf_stage2_bright = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='median')),\n",
        "    ('clf', RandomForestClassifier(\n",
        "        n_estimators=300, max_depth=None, min_samples_leaf=1,\n",
        "        n_jobs=-1, class_weight='balanced_subsample', random_state=SEED\n",
        "    ))\n",
        "])\n",
        "rf_stage2_bright.fit(X_train, y_train)\n",
        "\n",
        "# 6) Scores & predictions (EX positive)\n",
        "ex_idx = list(rf_stage2_bright.named_steps['clf'].classes_).index('EX')\n",
        "y_scores = rf_stage2_bright.predict_proba(X_test)[:, ex_idx]\n",
        "y_pred   = np.where(y_scores >= 0.5, 'EX', 'CWS')\n",
        "\n",
        "# 7) Metrics\n",
        "y_true_bin = (y_test == 'EX').astype(int)\n",
        "y_pred_bin = (y_pred == 'EX').astype(int)\n",
        "tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin).ravel()\n",
        "sensitivity = tp/(tp+fn) if (tp+fn) else 0.0\n",
        "specificity = tn/(tn+fp) if (tn+fp) else 0.0\n",
        "accuracy   = (tp+tn)/(tp+tn+fp+fn)\n",
        "roc_auc    = roc_auc_score(y_true_bin, y_scores)\n",
        "pr_auc     = average_precision_score(y_true_bin, y_scores)\n",
        "\n",
        "print(\"\\n=== Stage 2 — Bright (EX vs CWS) — Random Forest ===\")\n",
        "print(f\"Sensitivity (EX+): {sensitivity:.4f}\")\n",
        "print(f\"Specificity (CWS): {specificity:.4f}\")\n",
        "print(f\"Accuracy:          {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC:           {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC (EX+):      {pr_auc:.4f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# 8) Confusion matrices\n",
        "labels = ['CWS','EX']\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(values_format='d')\n",
        "plt.title('Stage 2 Bright — Confusion Matrix (counts, RF)'); plt.show()\n",
        "\n",
        "cm_norm = confusion_matrix(y_test, y_pred, labels=labels, normalize='true')\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=labels).plot(values_format='.2f')\n",
        "plt.title('Stage 2 Bright — Confusion Matrix (row-normalized, RF)'); plt.show()\n",
        "\n",
        "# 9) ROC\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "fpr, tpr, _ = roc_curve(y_true_bin, y_scores)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {roc_auc:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--', linewidth=1)\n",
        "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
        "plt.title('Stage 2 Bright — ROC (RF)')\n",
        "plt.legend(loc='lower right'); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# 10) Save results\n",
        "results_rf_b2 = {\n",
        "    'stage':'Stage 2 — Bright (EX vs CWS)', 'model':'RandomForest',\n",
        "    'sensitivity':sensitivity,'specificity':specificity,'accuracy':accuracy,\n",
        "    'roc_auc':roc_auc,'pr_auc':pr_auc,'tp':tp,'tn':tn,'fp':fp,'fn':fn\n",
        "}\n"
      ],
      "metadata": {
        "id": "3IsNzE9IUY6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGBoost"
      ],
      "metadata": {
        "id": "d6dgkg7WUR35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 2 — Bright (EX vs CWS) — XGBoost (simple) =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                             roc_auc_score, average_precision_score,\n",
        "                             ConfusionMatrixDisplay, roc_curve, auc)\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "EX_TO_CWS_RATIO_IN_TRAIN = 2.0   # set None to skip\n",
        "\n",
        "# 1) Data (bright-only, EX vs CWS)\n",
        "b2 = df_main[df_main['lesion_group']=='bright'].copy()\n",
        "b2 = b2[b2['lesion_type'].isin(['EX','CWS'])].copy()\n",
        "b2['target'] = b2['lesion_type']\n",
        "\n",
        "# 2) Group-safe split\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
        "train_idx, test_idx = next(gss.split(b2, groups=b2['image']))\n",
        "train_df, test_df = b2.iloc[train_idx].copy(), b2.iloc[test_idx].copy()\n",
        "if test_df['target'].nunique() < 2:\n",
        "    train_idx, test_idx = next(GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED+1)\n",
        "                               .split(b2, groups=b2['image']))\n",
        "    train_df, test_df = b2.iloc[train_idx].copy(), b2.iloc[test_idx].copy()\n",
        "\n",
        "# 3) Train-only rebalance (EX downsample)\n",
        "if EX_TO_CWS_RATIO_IN_TRAIN is not None:\n",
        "    ex_tr  = train_df[train_df['target']=='EX']\n",
        "    cws_tr = train_df[train_df['target']=='CWS']\n",
        "    target_ex = int(min(len(ex_tr), EX_TO_CWS_RATIO_IN_TRAIN * len(cws_tr))) if len(cws_tr)>0 else len(ex_tr)\n",
        "    ex_down = ex_tr.sample(n=target_ex, random_state=SEED) if len(ex_tr) > target_ex else ex_tr\n",
        "    train_df = pd.concat([ex_down, cws_tr], ignore_index=True).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# 4) Features / labels\n",
        "drop_cols = ['image','x','y','lesion','lesion_group','lesion_type','target']\n",
        "feat_cols = [c for c in b2.columns if c not in drop_cols]\n",
        "X_train, y_train_lbl = train_df[feat_cols], train_df['target']\n",
        "X_test,  y_test_lbl  = test_df[feat_cols],  test_df['target']\n",
        "\n",
        "# Binarize (EX=1, CWS=0) for XGB\n",
        "y_train = (y_train_lbl == 'EX').astype(int).values\n",
        "y_test  = (y_test_lbl == 'EX').astype(int).values\n",
        "\n",
        "# 5) Impute (trees don't need scaling)\n",
        "imp = SimpleImputer(strategy='median')\n",
        "X_train_np = imp.fit_transform(X_train)\n",
        "X_test_np  = imp.transform(X_test)\n",
        "\n",
        "# 6) Imbalance weight from TRAIN\n",
        "pos = int(y_train.sum()); neg = int(len(y_train) - pos)\n",
        "scale_pos_weight = (neg / pos) if pos > 0 else 1.0\n",
        "print(f\"scale_pos_weight (train): {scale_pos_weight:.3f}  (neg={neg}, pos={pos})\")\n",
        "\n",
        "# 7) Model (simple; no early stopping to avoid version differences)\n",
        "xgb_stage2_bright = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    max_depth=6,\n",
        "    n_estimators=400,\n",
        "    learning_rate=0.10,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    n_jobs=-1,\n",
        "    random_state=SEED\n",
        ")\n",
        "xgb_stage2_bright.fit(X_train_np, y_train)\n",
        "\n",
        "# 8) Scores & predictions (EX positive)\n",
        "y_scores = xgb_stage2_bright.predict_proba(X_test_np)[:, 1]\n",
        "y_pred_bin = (y_scores >= 0.5).astype(int)\n",
        "y_pred_lbl = np.where(y_pred_bin==1, 'EX', 'CWS')\n",
        "\n",
        "# 9) Metrics\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_bin).ravel()\n",
        "sensitivity = tp/(tp+fn) if (tp+fn) else 0.0\n",
        "specificity = tn/(tn+fp) if (tn+fp) else 0.0\n",
        "accuracy   = (tp+tn)/(tp+tn+fp+fn)\n",
        "roc_auc    = roc_auc_score(y_test, y_scores)\n",
        "pr_auc     = average_precision_score(y_test, y_scores)\n",
        "\n",
        "print(\"\\n=== Stage 2 — Bright (EX vs CWS) — XGBoost (simple) ===\")\n",
        "print(f\"Sensitivity (EX+): {sensitivity:.4f}\")\n",
        "print(f\"Specificity (CWS): {specificity:.4f}\")\n",
        "print(f\"Accuracy:          {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC:           {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC (EX+):      {pr_auc:.4f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test_lbl, y_pred_lbl, digits=4))\n",
        "\n",
        "# 10) Confusion matrices\n",
        "labels = ['CWS','EX']\n",
        "cm = confusion_matrix(y_test_lbl, y_pred_lbl, labels=labels)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(values_format='d')\n",
        "plt.title('Stage 2 Bright — Confusion Matrix (counts, XGB)'); plt.show()\n",
        "\n",
        "cm_norm = confusion_matrix(y_test_lbl, y_pred_lbl, labels=labels, normalize='true')\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=labels).plot(values_format='.2f')\n",
        "plt.title('Stage 2 Bright — Confusion Matrix (row-normalized, XGB)'); plt.show()\n",
        "\n",
        "# 11) ROC\n",
        "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {roc_auc:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--', linewidth=1)\n",
        "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
        "plt.title('Stage 2 Bright — ROC (XGBoost)')\n",
        "plt.legend(loc='lower right'); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# 12) Save results\n",
        "results_xgb_b2 = {\n",
        "    'stage':'Stage 2 — Bright (EX vs CWS)', 'model':'XGBoost',\n",
        "    'sensitivity':sensitivity,'specificity':specificity,'accuracy':accuracy,\n",
        "    'roc_auc':roc_auc,'pr_auc':pr_auc,'tp':tp,'tn':tn,'fp':fp,'fn':fn\n",
        "}\n"
      ],
      "metadata": {
        "id": "4a8iypeQUZfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Comparison"
      ],
      "metadata": {
        "id": "vWCApfOvYbii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def make_model_comparison(rows, stage_name, sort_by=\"ROC AUC\"):\n",
        "    \"\"\"\n",
        "    rows: list of results_* dicts like {'model','sensitivity','specificity','accuracy','roc_auc','pr_auc','tp','fp','tn','fn'}\n",
        "    stage_name: string used for filenames/captions\n",
        "    sort_by: which metric to sort by (e.g., 'ROC AUC', 'Accuracy', 'Balanced Accuracy')\n",
        "    \"\"\"\n",
        "    tbl = pd.DataFrame([\n",
        "        {\n",
        "            \"Model\": r[\"model\"],\n",
        "            \"Sensitivity\": r[\"sensitivity\"],\n",
        "            \"Specificity\": r[\"specificity\"],\n",
        "            \"Balanced Accuracy\": 0.5 * (r[\"sensitivity\"] + r[\"specificity\"]),\n",
        "            \"Accuracy\": r[\"accuracy\"],\n",
        "            \"ROC AUC\": r[\"roc_auc\"],\n",
        "            \"PR AUC\": r[\"pr_auc\"],\n",
        "            \"TP\": r[\"tp\"], \"FP\": r[\"fp\"], \"TN\": r[\"tn\"], \"FN\": r[\"fn\"]\n",
        "        }\n",
        "        for r in rows\n",
        "    ]).round(4).sort_values(sort_by, ascending=False)\n",
        "\n",
        "    print(tbl)\n",
        "\n",
        "    # Optional pretty display in notebooks\n",
        "    try:\n",
        "        display(tbl.style.highlight_max(\n",
        "            subset=[\"Sensitivity\",\"Specificity\",\"Balanced Accuracy\",\"Accuracy\",\"ROC AUC\",\"PR AUC\"],\n",
        "            color=\"#d5f5e3\"\n",
        "        ))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Save\n",
        "    csv_name = stage_name.lower().replace(\" \", \"_\").replace(\"/\", \"_\") + \"_model_comparison.csv\"\n",
        "    tbl.to_csv(csv_name, index=False)\n",
        "\n",
        "    # LaTeX\n",
        "    print(\"\\nLaTeX (paste into your paper):\\n\")\n",
        "    print(tbl.to_latex(index=False, float_format=\"%.4f\",\n",
        "                       caption=f\"{stage_name} — Model comparison\",\n",
        "                       label=\"tab:\" + stage_name.lower().replace(\" \", \"_\").replace(\"/\", \"_\")))\n",
        "    return tbl\n",
        "\n",
        "# ==== Usage examples ====\n",
        "# Stage 1 Bright vs Background\n",
        "_ = make_model_comparison(\n",
        "    [results_lr_b1, results_rf_b1, results_xgb_b1],\n",
        "    stage_name=\"Stage 1 (Bright vs Background)\"\n",
        ")\n",
        "\n",
        "# Stage 2 Bright (EX vs CWS)\n",
        "_ = make_model_comparison(\n",
        "    [results_lr_b2, results_rf_b2, results_xgb_b2],\n",
        "    stage_name=\"Stage 2 Bright (EX vs CWS)\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "V_ZSqZmHVmzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 Red"
      ],
      "metadata": {
        "id": "84rQskg7C79T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2.1 Red vs Background"
      ],
      "metadata": {
        "id": "JoHEICZ0DCoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression"
      ],
      "metadata": {
        "id": "LZocUJSAWK4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 1 — Red vs Background (Logistic Regression) =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                             roc_auc_score, average_precision_score,\n",
        "                             ConfusionMatrixDisplay, roc_curve, auc)\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "N_BG_RED = 50_000  # undersample background (tweak for speed/balance)\n",
        "\n",
        "# 1) Build dataset (undersample background + all red)\n",
        "df_bg = df_main[df_main['lesion_group'] == 'background']\n",
        "df_red = df_main[df_main['lesion_group'] == 'red']\n",
        "df_bg_s = df_bg.sample(n=min(N_BG_RED, len(df_bg)), random_state=SEED)\n",
        "\n",
        "df_s1_red = pd.concat([df_bg_s, df_red], ignore_index=True)\n",
        "df_s1_red['target'] = np.where(df_s1_red['lesion_group']=='red', 'red', 'background')\n",
        "df_s1_red = df_s1_red.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# 2) Leakage-safe split by image\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
        "train_idx, test_idx = next(gss.split(df_s1_red, groups=df_s1_red['image']))\n",
        "train_df, test_df = df_s1_red.iloc[train_idx].copy(), df_s1_red.iloc[test_idx].copy()\n",
        "\n",
        "# 3) Features / labels\n",
        "drop_cols = ['image','x','y','lesion','lesion_group','lesion_type','target']\n",
        "feat_cols = [c for c in df_s1_red.columns if c not in drop_cols]\n",
        "X_train, y_train = train_df[feat_cols], train_df['target']\n",
        "X_test,  y_test  = test_df[feat_cols],  test_df['target']\n",
        "\n",
        "# 4) Model: Impute + Scale + Logistic Regression (balanced)\n",
        "lr_stage1_red = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='median')),\n",
        "    ('scale', StandardScaler()),\n",
        "    ('clf', LogisticRegression(\n",
        "        solver='lbfgs', max_iter=1000, class_weight='balanced',\n",
        "        C=1.0, random_state=SEED\n",
        "    ))\n",
        "])\n",
        "lr_stage1_red.fit(X_train, y_train)\n",
        "\n",
        "# 5) Scores & predictions (red = positive)\n",
        "red_idx = list(lr_stage1_red.named_steps['clf'].classes_).index('red')\n",
        "y_scores = lr_stage1_red.predict_proba(X_test)[:, red_idx]\n",
        "y_pred   = np.where(y_scores >= 0.5, 'red', 'background')\n",
        "\n",
        "# 6) Metrics\n",
        "y_true_bin = (y_test == 'red').astype(int)\n",
        "y_pred_bin = (y_pred == 'red').astype(int)\n",
        "tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin).ravel()\n",
        "sensitivity = tp/(tp+fn) if (tp+fn) else 0.0   # red recall\n",
        "specificity = tn/(tn+fp) if (tn+fp) else 0.0   # background TNR\n",
        "accuracy   = (tp+tn)/(tp+tn+fp+fn)\n",
        "roc_auc    = roc_auc_score(y_true_bin, y_scores)\n",
        "pr_auc     = average_precision_score(y_true_bin, y_scores)\n",
        "\n",
        "print(\"\\n=== Stage 1 — Red vs Background (LogReg) ===\")\n",
        "print(f\"Sensitivity (red+): {sensitivity:.4f}\")\n",
        "print(f\"Specificity (background): {specificity:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC (red+): {pr_auc:.4f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# 7) Confusion matrices\n",
        "labels = ['background','red']\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(values_format='d')\n",
        "plt.title('Stage 1 Red — Confusion Matrix (counts, LR)'); plt.show()\n",
        "\n",
        "cm_norm = confusion_matrix(y_test, y_pred, labels=labels, normalize='true')\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=labels).plot(values_format='.2f')\n",
        "plt.title('Stage 1 Red — Confusion Matrix (row-normalized, LR)'); plt.show()\n",
        "\n",
        "# 8) ROC\n",
        "fpr, tpr, _ = roc_curve(y_true_bin, y_scores)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {roc_auc:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--', linewidth=1)\n",
        "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
        "plt.title('Stage 1 Red — ROC (LR)'); plt.legend(loc='lower right'); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# 9) Save results\n",
        "results_lr_r1 = {\n",
        "    'stage': 'Stage 1 — Red vs Background', 'model': 'LogReg',\n",
        "    'sensitivity': sensitivity, 'specificity': specificity, 'accuracy': accuracy,\n",
        "    'roc_auc': roc_auc, 'pr_auc': pr_auc, 'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
        "}\n"
      ],
      "metadata": {
        "id": "j6u05IHLWQF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest"
      ],
      "metadata": {
        "id": "nwJJ9aCgWKxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 1 — Red vs Background (Random Forest) =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                             roc_auc_score, average_precision_score,\n",
        "                             ConfusionMatrixDisplay, roc_curve, auc)\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "N_BG_RED = 50_000\n",
        "\n",
        "# 1) Build dataset\n",
        "df_bg = df_main[df_main['lesion_group'] == 'background']\n",
        "df_red = df_main[df_main['lesion_group'] == 'red']\n",
        "df_bg_s = df_bg.sample(n=min(N_BG_RED, len(df_bg)), random_state=SEED)\n",
        "\n",
        "df_s1_red = pd.concat([df_bg_s, df_red], ignore_index=True)\n",
        "df_s1_red['target'] = np.where(df_s1_red['lesion_group']=='red', 'red', 'background')\n",
        "df_s1_red = df_s1_red.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# 2) Group-safe split\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
        "train_idx, test_idx = next(gss.split(df_s1_red, groups=df_s1_red['image']))\n",
        "train_df, test_df = df_s1_red.iloc[train_idx].copy(), df_s1_red.iloc[test_idx].copy()\n",
        "\n",
        "# 3) Features / labels\n",
        "drop_cols = ['image','x','y','lesion','lesion_group','lesion_type','target']\n",
        "feat_cols = [c for c in df_s1_red.columns if c not in drop_cols]\n",
        "X_train, y_train = train_df[feat_cols], train_df['target']\n",
        "X_test,  y_test  = test_df[feat_cols],  test_df['target']\n",
        "\n",
        "# 4) Model: Impute -> RF (balanced subsample)\n",
        "rf_stage1_red = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='median')),\n",
        "    ('clf', RandomForestClassifier(\n",
        "        n_estimators=300, max_depth=None, min_samples_leaf=1,\n",
        "        n_jobs=-1, class_weight='balanced_subsample', random_state=SEED\n",
        "    ))\n",
        "])\n",
        "rf_stage1_red.fit(X_train, y_train)\n",
        "\n",
        "# 5) Scores & predictions (red positive)\n",
        "red_idx = list(rf_stage1_red.named_steps['clf'].classes_).index('red')\n",
        "y_scores = rf_stage1_red.predict_proba(X_test)[:, red_idx]\n",
        "y_pred   = np.where(y_scores >= 0.5, 'red', 'background')\n",
        "\n",
        "# 6) Metrics\n",
        "y_true_bin = (y_test == 'red').astype(int)\n",
        "y_pred_bin = (y_pred == 'red').astype(int)\n",
        "tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin).ravel()\n",
        "sensitivity = tp/(tp+fn) if (tp+fn) else 0.0\n",
        "specificity = tn/(tn+fp) if (tn+fp) else 0.0\n",
        "accuracy   = (tp+tn)/(tp+tn+fp+fn)\n",
        "roc_auc    = roc_auc_score(y_true_bin, y_scores)\n",
        "pr_auc     = average_precision_score(y_true_bin, y_scores)\n",
        "\n",
        "print(\"\\n=== Stage 1 — Red vs Background (Random Forest) ===\")\n",
        "print(f\"Sensitivity (red+): {sensitivity:.4f}\")\n",
        "print(f\"Specificity (background): {specificity:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC (red+): {pr_auc:.4f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# 7) Confusion matrices\n",
        "labels = ['background','red']\n",
        "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(values_format='d')\n",
        "plt.title('Stage 1 Red — Confusion Matrix (counts, RF)'); plt.show()\n",
        "\n",
        "cm_norm = confusion_matrix(y_test, y_pred, labels=labels, normalize='true')\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=labels).plot(values_format='.2f')\n",
        "plt.title('Stage 1 Red — Confusion Matrix (row-normalized, RF)'); plt.show()\n",
        "\n",
        "# 8) ROC\n",
        "fpr, tpr, _ = roc_curve(y_true_bin, y_scores)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {roc_auc:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--', linewidth=1)\n",
        "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
        "plt.title('Stage 1 Red — ROC (RF)'); plt.legend(loc='lower right'); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# 9) Save results\n",
        "results_rf_r1 = {\n",
        "    'stage': 'Stage 1 — Red vs Background', 'model': 'RandomForest',\n",
        "    'sensitivity': sensitivity, 'specificity': specificity, 'accuracy': accuracy,\n",
        "    'roc_auc': roc_auc, 'pr_auc': pr_auc, 'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
        "}\n"
      ],
      "metadata": {
        "id": "FL-avb1tWd87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGBoost"
      ],
      "metadata": {
        "id": "-nbAnHP-WKpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 1 — Red vs Background (XGBoost — simple) =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                             roc_auc_score, average_precision_score,\n",
        "                             ConfusionMatrixDisplay, roc_curve, auc)\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "N_BG_RED = 50_000  # undersample background\n",
        "\n",
        "# 1) Build dataset\n",
        "df_bg = df_main[df_main['lesion_group'] == 'background']\n",
        "df_red = df_main[df_main['lesion_group'] == 'red']\n",
        "df_bg_s = df_bg.sample(n=min(N_BG_RED, len(df_bg)), random_state=SEED)\n",
        "\n",
        "df_s1_red = pd.concat([df_bg_s, df_red], ignore_index=True)\n",
        "df_s1_red['target'] = np.where(df_s1_red['lesion_group']=='red', 'red', 'background')\n",
        "df_s1_red = df_s1_red.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# 2) Group-safe split\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
        "train_idx, test_idx = next(gss.split(df_s1_red, groups=df_s1_red['image']))\n",
        "train_df, test_df = df_s1_red.iloc[train_idx].copy(), df_s1_red.iloc[test_idx].copy()\n",
        "\n",
        "# 3) Features / labels, binarize (red=1)\n",
        "drop_cols = ['image','x','y','lesion','lesion_group','lesion_type','target']\n",
        "feat_cols = [c for c in df_s1_red.columns if c not in drop_cols]\n",
        "X_train, y_train_lbl = train_df[feat_cols], train_df['target']\n",
        "X_test,  y_test_lbl  = test_df[feat_cols],  test_df['target']\n",
        "\n",
        "y_train = (y_train_lbl == 'red').astype(int).values\n",
        "y_test  = (y_test_lbl == 'red').astype(int).values\n",
        "\n",
        "# 4) Impute (trees don't need scaling)\n",
        "imp = SimpleImputer(strategy='median')\n",
        "X_train_np = imp.fit_transform(X_train)\n",
        "X_test_np  = imp.transform(X_test)\n",
        "\n",
        "# 5) Imbalance weight from TRAIN\n",
        "pos = int(y_train.sum()); neg = int(len(y_train) - pos)\n",
        "scale_pos_weight = (neg / pos) if pos > 0 else 1.0\n",
        "print(f\"scale_pos_weight (train): {scale_pos_weight:.3f}  (neg={neg}, pos={pos})\")\n",
        "\n",
        "# 6) Model (simple; no early stopping)\n",
        "xgb_stage1_red = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    max_depth=6,\n",
        "    n_estimators=400,\n",
        "    learning_rate=0.10,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    n_jobs=-1,\n",
        "    random_state=SEED\n",
        ")\n",
        "xgb_stage1_red.fit(X_train_np, y_train)\n",
        "\n",
        "# 7) Scores & predictions (red positive)\n",
        "y_scores = xgb_stage1_red.predict_proba(X_test_np)[:, 1]\n",
        "y_pred_bin = (y_scores >= 0.5).astype(int)\n",
        "y_pred_lbl = np.where(y_pred_bin==1, 'red', 'background')\n",
        "\n",
        "# 8) Metrics\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_bin).ravel()\n",
        "sensitivity = tp/(tp+fn) if (tp+fn) else 0.0\n",
        "specificity = tn/(tn+fp) if (tn+fp) else 0.0\n",
        "accuracy   = (tp+tn)/(tp+tn+fp+fn)\n",
        "roc_auc    = roc_auc_score(y_test, y_scores)\n",
        "pr_auc     = average_precision_score(y_test, y_scores)\n",
        "\n",
        "print(\"\\n=== Stage 1 — Red vs Background (XGBoost — simple) ===\")\n",
        "print(f\"Sensitivity (red+): {sensitivity:.4f}\")\n",
        "print(f\"Specificity (background): {specificity:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC (red+): {pr_auc:.4f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test_lbl, y_pred_lbl, digits=4))\n",
        "\n",
        "# 9) Confusion matrices\n",
        "labels = ['background','red']\n",
        "cm = confusion_matrix(y_test_lbl, y_pred_lbl, labels=labels)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(values_format='d')\n",
        "plt.title('Stage 1 Red — Confusion Matrix (counts, XGB)'); plt.show()\n",
        "\n",
        "cm_norm = confusion_matrix(y_test_lbl, y_pred_lbl, labels=labels, normalize='true')\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=labels).plot(values_format='.2f')\n",
        "plt.title('Stage 1 Red — Confusion Matrix (row-normalized, XGB)'); plt.show()\n",
        "\n",
        "# 10) ROC\n",
        "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {roc_auc:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--', linewidth=1)\n",
        "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
        "plt.title('Stage 1 Red — ROC (XGBoost)')\n",
        "plt.legend(loc='lower right'); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# 11) Save results\n",
        "results_xgb_r1 = {\n",
        "    'stage': 'Stage 1 — Red vs Background', 'model': 'XGBoost',\n",
        "    'sensitivity': sensitivity, 'specificity': specificity, 'accuracy': accuracy,\n",
        "    'roc_auc': roc_auc, 'pr_auc': pr_auc, 'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
        "}\n"
      ],
      "metadata": {
        "id": "BonB8WtjVmBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Comparison"
      ],
      "metadata": {
        "id": "8WYsF8N7YnE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Collect result dicts (LogReg / RF / XGB)\n",
        "rows = [results_lr_r1, results_rf_r1, results_xgb_r1]\n",
        "\n",
        "# Build comparison table\n",
        "tbl_r1 = pd.DataFrame([\n",
        "    {\n",
        "        \"Model\": r[\"model\"],\n",
        "        \"Sensitivity (red)\": r[\"sensitivity\"],\n",
        "        \"Specificity (background)\": r[\"specificity\"],\n",
        "        \"Balanced Accuracy\": 0.5 * (r[\"sensitivity\"] + r[\"specificity\"]),\n",
        "        \"Accuracy\": r[\"accuracy\"],\n",
        "        \"ROC AUC\": r[\"roc_auc\"],\n",
        "        \"PR AUC (red)\": r[\"pr_auc\"],\n",
        "        \"TP\": r[\"tp\"], \"FP\": r[\"fp\"], \"TN\": r[\"tn\"], \"FN\": r[\"fn\"]\n",
        "    }\n",
        "    for r in rows\n",
        "]).round(4).sort_values(\"ROC AUC\", ascending=False)\n",
        "\n",
        "# Paper’s AUC definition (avg of sensitivity & specificity)\n",
        "tbl_r1[\"AUC (paper def)\"] = tbl_r1[\"Balanced Accuracy\"]\n",
        "\n",
        "print(tbl_r1)\n",
        "\n",
        "# Optional pretty display (Jupyter)\n",
        "try:\n",
        "    display(\n",
        "        tbl_r1.style.highlight_max(\n",
        "            subset=[\"Sensitivity (red)\", \"Specificity (background)\",\n",
        "                    \"Balanced Accuracy\", \"Accuracy\", \"ROC AUC\", \"PR AUC (red)\"],\n",
        "            color=\"#d5f5e3\"\n",
        "        )\n",
        "    )\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Export for your report\n",
        "tbl_r1.to_csv(\"stage1_red_model_comparison.csv\", index=False)\n",
        "print(\"\\nLaTeX (paste into your paper):\\n\")\n",
        "print(tbl_r1.to_latex(\n",
        "    index=False,\n",
        "    float_format=\"%.4f\",\n",
        "    caption=\"Stage 1 (Red vs Background) — Model comparison\",\n",
        "    label=\"tab:s1_red_models\"\n",
        "))\n"
      ],
      "metadata": {
        "id": "j9Ki9O41WtM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2.2 Red: MA vs HM"
      ],
      "metadata": {
        "id": "1V_xG5XUDEII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression"
      ],
      "metadata": {
        "id": "92l5abrGW9Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 2 — Red (MA vs HM) — Logistic Regression =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                             roc_auc_score, average_precision_score,\n",
        "                             ConfusionMatrixDisplay, roc_curve, auc)\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "HM_TO_MA_RATIO_IN_TRAIN = 2.0   # set None to skip train-only rebalance\n",
        "\n",
        "# 1) Data: red-only, normalize HE->HM, keep MA/HM\n",
        "r2 = df_main[df_main['lesion_group']=='red'].copy()\n",
        "r2['lesion_type'] = r2['lesion_type'].replace({'HE':'HM'})\n",
        "r2 = r2[r2['lesion_type'].isin(['MA','HM'])].copy()\n",
        "r2['target'] = r2['lesion_type']\n",
        "\n",
        "# 2) Group-safe split by image (+ ensure both classes in test)\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
        "train_idx, test_idx = next(gss.split(r2, groups=r2['image']))\n",
        "train_df, test_df = r2.iloc[train_idx].copy(), r2.iloc[test_idx].copy()\n",
        "if test_df['target'].nunique() < 2:\n",
        "    train_idx, test_idx = next(GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED+1)\n",
        "                               .split(r2, groups=r2['image']))\n",
        "    train_df, test_df = r2.iloc[train_idx].copy(), r2.iloc[test_idx].copy()\n",
        "\n",
        "# 3) Train-only rebalance: undersample HM to ratio × MA\n",
        "if HM_TO_MA_RATIO_IN_TRAIN is not None:\n",
        "    hm_tr = train_df[train_df['target']=='HM']\n",
        "    ma_tr = train_df[train_df['target']=='MA']\n",
        "    target_hm = int(min(len(hm_tr), HM_TO_MA_RATIO_IN_TRAIN * len(ma_tr))) if len(ma_tr)>0 else len(hm_tr)\n",
        "    hm_down = hm_tr.sample(n=target_hm, random_state=SEED) if len(hm_tr) > target_hm else hm_tr\n",
        "    train_df = pd.concat([hm_down, ma_tr], ignore_index=True).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# 4) Features / labels\n",
        "drop_cols = ['image','x','y','lesion','lesion_group','lesion_type','target']\n",
        "feat_cols = [c for c in r2.columns if c not in drop_cols]\n",
        "X_train, y_train = train_df[feat_cols], train_df['target']\n",
        "X_test,  y_test  = test_df[feat_cols],  test_df['target']\n",
        "\n",
        "# 5) Model: Impute + Scale + LogReg (balanced)\n",
        "lr_stage2_red = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='median')),\n",
        "    ('scale', StandardScaler()),\n",
        "    ('clf', LogisticRegression(\n",
        "        solver='lbfgs', max_iter=1000, class_weight='balanced',\n",
        "        C=1.0, random_state=SEED\n",
        "    ))\n",
        "]).fit(X_train, y_train)\n",
        "\n",
        "# 6) Scores & preds (MA positive)\n",
        "ma_idx = list(lr_stage2_red.named_steps['clf'].classes_).index('MA')\n",
        "y_scores = lr_stage2_red.predict_proba(X_test)[:, ma_idx]\n",
        "y_pred   = np.where(y_scores >= 0.5, 'MA', 'HM')\n",
        "\n",
        "# 7) Metrics\n",
        "y_true_bin = (y_test == 'MA').astype(int); y_pred_bin = (y_pred == 'MA').astype(int)\n",
        "tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin).ravel()\n",
        "sens = tp/(tp+fn) if (tp+fn) else 0.0\n",
        "spec = tn/(tn+fp) if (tn+fp) else 0.0\n",
        "acc  = (tp+tn)/(tp+tn+fp+fn)\n",
        "rocA = roc_auc_score(y_true_bin, y_scores)\n",
        "prA  = average_precision_score(y_true_bin, y_scores)\n",
        "\n",
        "print(\"\\n=== Stage 2 — Red (MA vs HM) — Logistic Regression ===\")\n",
        "print(f\"Sensitivity (MA+): {sens:.4f}  Specificity (HM): {spec:.4f}  Acc: {acc:.4f}  ROC-AUC: {rocA:.4f}  PR-AUC: {prA:.4f}\")\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# 8) Plots\n",
        "labels = ['HM','MA']\n",
        "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred, labels=labels),\n",
        "                       display_labels=labels).plot(values_format='d')\n",
        "plt.title('Stage 2 Red — Confusion Matrix (counts, LR)'); plt.show()\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred, labels=labels, normalize='true'),\n",
        "                       display_labels=labels).plot(values_format='.2f')\n",
        "plt.title('Stage 2 Red — Confusion Matrix (row-normalized, LR)'); plt.show()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_true_bin, y_scores)\n",
        "plt.figure(figsize=(5,4)); plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {rocA:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--', linewidth=1); plt.xlabel('FPR'); plt.ylabel('TPR')\n",
        "plt.title('Stage 2 Red — ROC (LR)'); plt.legend(loc='lower right'); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# 9) Save results\n",
        "results_lr_r2 = {'stage':'Stage 2 — Red (MA vs HM)','model':'LogReg',\n",
        "                 'sensitivity':sens,'specificity':spec,'accuracy':acc,\n",
        "                 'roc_auc':rocA,'pr_auc':prA,'tp':tp,'tn':tn,'fp':fp,'fn':fn}\n"
      ],
      "metadata": {
        "id": "bPxM4ymcOcY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest"
      ],
      "metadata": {
        "id": "NBRvjVx_XAd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 2 — Red (MA vs HM) — Random Forest =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                             roc_auc_score, average_precision_score,\n",
        "                             ConfusionMatrixDisplay, roc_curve, auc)\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "HM_TO_MA_RATIO_IN_TRAIN = 2.0\n",
        "\n",
        "# 1) Data\n",
        "r2 = df_main[df_main['lesion_group']=='red'].copy()\n",
        "r2['lesion_type'] = r2['lesion_type'].replace({'HE':'HM'})\n",
        "r2 = r2[r2['lesion_type'].isin(['MA','HM'])].copy()\n",
        "r2['target'] = r2['lesion_type']\n",
        "\n",
        "# 2) Split (group-safe)\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
        "train_idx, test_idx = next(gss.split(r2, groups=r2['image']))\n",
        "train_df, test_df = r2.iloc[train_idx].copy(), r2.iloc[test_idx].copy()\n",
        "if test_df['target'].nunique() < 2:\n",
        "    train_idx, test_idx = next(GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED+1)\n",
        "                               .split(r2, groups=r2['image']))\n",
        "    train_df, test_df = r2.iloc[train_idx].copy(), r2.iloc[test_idx].copy()\n",
        "\n",
        "# 3) Train-only rebalance\n",
        "if HM_TO_MA_RATIO_IN_TRAIN is not None:\n",
        "    hm_tr = train_df[train_df['target']=='HM']\n",
        "    ma_tr = train_df[train_df['target']=='MA']\n",
        "    target_hm = int(min(len(hm_tr), HM_TO_MA_RATIO_IN_TRAIN * len(ma_tr))) if len(ma_tr)>0 else len(hm_tr)\n",
        "    hm_down = hm_tr.sample(n=target_hm, random_state=SEED) if len(hm_tr) > target_hm else hm_tr\n",
        "    train_df = pd.concat([hm_down, ma_tr], ignore_index=True).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# 4) Features / labels\n",
        "drop_cols = ['image','x','y','lesion','lesion_group','lesion_type','target']\n",
        "feat_cols = [c for c in r2.columns if c not in drop_cols]\n",
        "X_train, y_train = train_df[feat_cols], train_df['target']\n",
        "X_test,  y_test  = test_df[feat_cols],  test_df['target']\n",
        "\n",
        "# 5) Model\n",
        "rf_stage2_red = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='median')),\n",
        "    ('clf', RandomForestClassifier(\n",
        "        n_estimators=300, max_depth=None, min_samples_leaf=1,\n",
        "        n_jobs=-1, class_weight='balanced_subsample', random_state=SEED\n",
        "    ))\n",
        "]).fit(X_train, y_train)\n",
        "\n",
        "# 6) Scores & preds (MA positive)\n",
        "ma_idx = list(rf_stage2_red.named_steps['clf'].classes_).index('MA')\n",
        "y_scores = rf_stage2_red.predict_proba(X_test)[:, ma_idx]\n",
        "y_pred   = np.where(y_scores >= 0.5, 'MA', 'HM')\n",
        "\n",
        "# 7) Metrics\n",
        "y_true_bin = (y_test == 'MA').astype(int); y_pred_bin = (y_pred == 'MA').astype(int)\n",
        "tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin).ravel()\n",
        "sens = tp/(tp+fn) if (tp+fn) else 0.0\n",
        "spec = tn/(tn+fp) if (tn+fp) else 0.0\n",
        "acc  = (tp+tn)/(tp+tn+fp+fn)\n",
        "rocA = roc_auc_score(y_true_bin, y_scores)\n",
        "prA  = average_precision_score(y_true_bin, y_scores)\n",
        "\n",
        "print(\"\\n=== Stage 2 — Red (MA vs HM) — Random Forest ===\")\n",
        "print(f\"Sensitivity (MA+): {sens:.4f}  Specificity (HM): {spec:.4f}  Acc: {acc:.4f}  ROC-AUC: {rocA:.4f}  PR-AUC: {prA:.4f}\")\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# 8) Plots\n",
        "labels = ['HM','MA']\n",
        "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred, labels=labels),\n",
        "                       display_labels=labels).plot(values_format='d')\n",
        "plt.title('Stage 2 Red — Confusion Matrix (counts, RF)'); plt.show()\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred, labels=labels, normalize='true'),\n",
        "                       display_labels=labels).plot(values_format='.2f')\n",
        "plt.title('Stage 2 Red — Confusion Matrix (row-normalized, RF)'); plt.show()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_true_bin, y_scores)\n",
        "plt.figure(figsize=(5,4)); plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {rocA:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--', linewidth=1); plt.xlabel('FPR'); plt.ylabel('TPR')\n",
        "plt.title('Stage 2 Red — ROC (RF)'); plt.legend(loc='lower right'); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# 9) Save results\n",
        "results_rf_r2 = {'stage':'Stage 2 — Red (MA vs HM)','model':'RandomForest',\n",
        "                 'sensitivity':sens,'specificity':spec,'accuracy':acc,\n",
        "                 'roc_auc':rocA,'pr_auc':prA,'tp':tp,'tn':tn,'fp':fp,'fn':fn}\n"
      ],
      "metadata": {
        "id": "JTRNuxHQW_cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGBoost"
      ],
      "metadata": {
        "id": "m55Gr_stW_K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage 2 — Red (MA vs HM) — XGBoost (simple) =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                             roc_auc_score, average_precision_score,\n",
        "                             ConfusionMatrixDisplay, roc_curve, auc)\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.20\n",
        "HM_TO_MA_RATIO_IN_TRAIN = 2.0  # set None to skip rebalance\n",
        "\n",
        "# 1) Data\n",
        "r2 = df_main[df_main['lesion_group']=='red'].copy()\n",
        "r2['lesion_type'] = r2['lesion_type'].replace({'HE':'HM'})\n",
        "r2 = r2[r2['lesion_type'].isin(['MA','HM'])].copy()\n",
        "r2['target'] = r2['lesion_type']\n",
        "\n",
        "# 2) Split (group-safe)\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
        "train_idx, test_idx = next(gss.split(r2, groups=r2['image']))\n",
        "train_df, test_df = r2.iloc[train_idx].copy(), r2.iloc[test_idx].copy()\n",
        "if test_df['target'].nunique() < 2:\n",
        "    train_idx, test_idx = next(GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED+1)\n",
        "                               .split(r2, groups=r2['image']))\n",
        "    train_df, test_df = r2.iloc[train_idx].copy(), r2.iloc[test_idx].copy()\n",
        "\n",
        "# 3) Train-only rebalance\n",
        "if HM_TO_MA_RATIO_IN_TRAIN is not None:\n",
        "    hm_tr = train_df[train_df['target']=='HM']\n",
        "    ma_tr = train_df[train_df['target']=='MA']\n",
        "    target_hm = int(min(len(hm_tr), HM_TO_MA_RATIO_IN_TRAIN * len(ma_tr))) if len(ma_tr)>0 else len(hm_tr)\n",
        "    hm_down = hm_tr.sample(n=target_hm, random_state=SEED) if len(hm_tr) > target_hm else hm_tr\n",
        "    train_df = pd.concat([hm_down, ma_tr], ignore_index=True).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# 4) Features / labels (MA=1, HM=0)\n",
        "drop_cols = ['image','x','y','lesion','lesion_group','lesion_type','target']\n",
        "feat_cols = [c for c in r2.columns if c not in drop_cols]\n",
        "X_train, y_train_lbl = train_df[feat_cols], train_df['target']\n",
        "X_test,  y_test_lbl  = test_df[feat_cols],  test_df['target']\n",
        "\n",
        "y_train = (y_train_lbl == 'MA').astype(int).values\n",
        "y_test  = (y_test_lbl == 'MA').astype(int).values\n",
        "\n",
        "# 5) Impute (trees don’t need scaling)\n",
        "imp = SimpleImputer(strategy='median')\n",
        "X_train_np = imp.fit_transform(X_train)\n",
        "X_test_np  = imp.transform(X_test)\n",
        "\n",
        "# 6) Imbalance weight from TRAIN\n",
        "pos = int(y_train.sum()); neg = int(len(y_train) - pos)\n",
        "scale_pos_weight = (neg / pos) if pos > 0 else 1.0\n",
        "print(f\"scale_pos_weight (train): {scale_pos_weight:.3f}  (neg={neg}, pos={pos})\")\n",
        "\n",
        "# 7) Model (simple; no early stopping)\n",
        "xgb_stage2_red = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    max_depth=6,\n",
        "    n_estimators=400,\n",
        "    learning_rate=0.10,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    n_jobs=-1,\n",
        "    random_state=SEED\n",
        ").fit(X_train_np, y_train)\n",
        "\n",
        "# 8) Scores & preds (MA positive)\n",
        "y_scores = xgb_stage2_red.predict_proba(X_test_np)[:, 1]\n",
        "y_pred_bin = (y_scores >= 0.5).astype(int)\n",
        "y_pred_lbl = np.where(y_pred_bin==1, 'MA', 'HM')\n",
        "\n",
        "# 9) Metrics\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_bin).ravel()\n",
        "sens = tp/(tp+fn) if (tp+fn) else 0.0\n",
        "spec = tn/(tn+fp) if (tn+fp) else 0.0\n",
        "acc  = (tp+tn)/(tp+tn+fp+fn)\n",
        "rocA = roc_auc_score(y_test, y_scores)\n",
        "prA  = average_precision_score(y_test, y_scores)\n",
        "\n",
        "print(\"\\n=== Stage 2 — Red (MA vs HM) — XGBoost (simple) ===\")\n",
        "print(f\"Sensitivity (MA+): {sens:.4f}  Specificity (HM): {spec:.4f}  Acc: {acc:.4f}  ROC-AUC: {rocA:.4f}  PR-AUC: {prA:.4f}\")\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test_lbl, y_pred_lbl, digits=4))\n",
        "\n",
        "# 10) Plots\n",
        "labels = ['HM','MA']\n",
        "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test_lbl, y_pred_lbl, labels=labels),\n",
        "                       display_labels=labels).plot(values_format='d')\n",
        "plt.title('Stage 2 Red — Confusion Matrix (counts, XGB)'); plt.show()\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test_lbl, y_pred_lbl, labels=labels, normalize='true'),\n",
        "                       display_labels=labels).plot(values_format='.2f')\n",
        "plt.title('Stage 2 Red — Confusion Matrix (row-normalized, XGB)'); plt.show()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
        "plt.figure(figsize=(5,4)); plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {rocA:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--', linewidth=1); plt.xlabel('FPR'); plt.ylabel('TPR')\n",
        "plt.title('Stage 2 Red — ROC (XGBoost)'); plt.legend(loc='lower right'); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# 11) Save results\n",
        "results_xgb_r2 = {'stage':'Stage 2 — Red (MA vs HM)','model':'XGBoost',\n",
        "                  'sensitivity':sens,'specificity':spec,'accuracy':acc,\n",
        "                  'roc_auc':rocA,'pr_auc':prA,'tp':tp,'tn':tn,'fp':fp,'fn':fn}\n"
      ],
      "metadata": {
        "id": "wCW0FOXJXEFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Comparison"
      ],
      "metadata": {
        "id": "6udpQxD0YzCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "rows = [results_lr_r2, results_rf_r2, results_xgb_r2]\n",
        "\n",
        "tbl_r2 = pd.DataFrame([\n",
        "    {\n",
        "        \"Model\": r[\"model\"],\n",
        "        \"Sensitivity (MA)\": r[\"sensitivity\"],\n",
        "        \"Specificity (HM)\": r[\"specificity\"],\n",
        "        \"Balanced Accuracy\": 0.5 * (r[\"sensitivity\"] + r[\"specificity\"]),\n",
        "        \"Accuracy\": r[\"accuracy\"],\n",
        "        \"ROC AUC\": r[\"roc_auc\"],\n",
        "        \"PR AUC (MA)\": r[\"pr_auc\"],\n",
        "        \"TP\": r[\"tp\"], \"FP\": r[\"fp\"], \"TN\": r[\"tn\"], \"FN\": r[\"fn\"]\n",
        "    }\n",
        "    for r in rows\n",
        "]).round(4).sort_values(\"ROC AUC\", ascending=False)\n",
        "\n",
        "# Paper’s AUC definition (avg of sensitivity & specificity)\n",
        "tbl_r2[\"AUC (paper def)\"] = tbl_r2[\"Balanced Accuracy\"]\n",
        "\n",
        "print(tbl_r2)\n",
        "\n",
        "# Optional pretty display (Jupyter)\n",
        "try:\n",
        "    display(tbl_r2.style.highlight_max(\n",
        "        subset=[\"Sensitivity (MA)\",\"Specificity (HM)\",\"Balanced Accuracy\",\"Accuracy\",\"ROC AUC\",\"PR AUC (MA)\"],\n",
        "        color=\"#d5f5e3\"\n",
        "    ))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Export for your report\n",
        "tbl_r2.to_csv(\"stage2_red_model_comparison.csv\", index=False)\n",
        "print(\"\\nLaTeX (paste into your paper):\\n\")\n",
        "print(tbl_r2.to_latex(index=False, float_format=\"%.4f\",\n",
        "                      caption=\"Stage 2 (Red: MA vs HM) — Model comparison\",\n",
        "                      label=\"tab:s2_red_models\"))\n"
      ],
      "metadata": {
        "id": "jb0y_jP-Xf0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KYB_WhggXixd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1EL1gF21jLap",
        "ERLtHEHSqYsW",
        "Rjm15Vitvpkz",
        "hdUaosms_Cjz",
        "yc1Otu2ahWVZ",
        "V6cV9Zlmqkst",
        "TidTa9eqs7oI",
        "xxOQiM2qn72I",
        "0DPgnqjnoJbn",
        "RI2smQCLfl9T",
        "Vs5p598NftQ-",
        "VSvVngcpfzgA",
        "HKkV-bmRp5G9",
        "SwqTq5FEw9gt",
        "sGZ2LUyOm87O",
        "QCJmlwSon9ij",
        "P7v1XBjojct1",
        "oM2-i-nIEviK",
        "GZL6iymRlGjB",
        "9Z7hNvOFlX7e",
        "20oa9c77IO5B",
        "qgUvEr4_Qjyb",
        "S2-yfMDoQeKu",
        "bNXeVrcJQrvi",
        "FqtMbVvajkjr",
        "NsOhfA-sCzih",
        "dJZ3ww51UR-l",
        "-FGHip7lUR69",
        "d6dgkg7WUR35",
        "vWCApfOvYbii",
        "84rQskg7C79T",
        "JoHEICZ0DCoF",
        "LZocUJSAWK4Y",
        "nwJJ9aCgWKxc",
        "-nbAnHP-WKpV",
        "8WYsF8N7YnE4",
        "1V_xG5XUDEII",
        "92l5abrGW9Nq",
        "NBRvjVx_XAd-",
        "m55Gr_stW_K7",
        "6udpQxD0YzCN"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}